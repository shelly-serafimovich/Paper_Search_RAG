query,expanded_query,title,abstract,model_score,cosine_similarity,source_query,avg_original_score,avg_expanded_score,overlap_count
What are the latest techniques for explainability in deep neural networks?,"What are the latest techniques for explainability in deep neural networks? - Interpretability
- Model transparency
- Feature importance
- Explainable AI
- XAI techniques
- White-box models
- Black-box models
- Post-hoc explainability
- Local interpretability
- Global interpretability","Robust Explainability: A Tutorial on Gradient-Based Attribution Methods
  for Deep Neural Networks","  With the rise of deep neural networks, the challenge of explaining the
predictions of these networks has become increasingly recognized. While many
methods for explaining the decisions of deep neural networks exist, there is
currently no consensus on how to evaluate them. On the other hand, robustness
is a popular topic for deep learning research; however, it is hardly talked
about in explainability until very recently. In this tutorial paper, we start
by presenting gradient-based interpretability methods. These techniques use
gradient signals to assign the burden of the decision on the input features.
Later, we discuss how gradient-based methods can be evaluated for their
robustness and the role that adversarial robustness plays in having meaningful
explanations. We also discuss the limitations of gradient-based methods.
Finally, we present the best practices and attributes that should be examined
before choosing an explainability method. We conclude with the future
directions for research in the area at the convergence of robustness and
explainability.
",8.666666666666666,0.7853645,overlap,6.8,7.333333333333334,4
What are the latest techniques for explainability in deep neural networks?,"What are the latest techniques for explainability in deep neural networks? - Interpretability
- Model transparency
- Feature importance
- Explainable AI
- XAI techniques
- White-box models
- Black-box models
- Post-hoc explainability
- Local interpretability
- Global interpretability","Explaining Deep Neural Networks and Beyond: A Review of Methods and
  Applications","  With the broader and highly successful usage of machine learning in industry
and the sciences, there has been a growing demand for Explainable AI.
Interpretability and explanation methods for gaining a better understanding
about the problem solving abilities and strategies of nonlinear Machine
Learning, in particular, deep neural networks, are therefore receiving
increased attention. In this work we aim to (1) provide a timely overview of
this active emerging field, with a focus on 'post-hoc' explanations, and
explain its theoretical foundations, (2) put interpretability algorithms to a
test both from a theory and comparative evaluation perspective using extensive
simulations, (3) outline best practice aspects i.e. how to best include
interpretation methods into the standard usage of machine learning and (4)
demonstrate successful usage of explainable AI in a representative selection of
application scenarios. Finally, we discuss challenges and possible future
directions of this exciting foundational field of machine learning.
",9.333333333333334,0.72521746,overlap,6.8,7.333333333333334,4
What are the latest techniques for explainability in deep neural networks?,"What are the latest techniques for explainability in deep neural networks? - Interpretability
- Model transparency
- Feature importance
- Explainable AI
- XAI techniques
- White-box models
- Black-box models
- Post-hoc explainability
- Local interpretability
- Global interpretability",Explaining Deep Neural Networks,"  Deep neural networks are becoming more and more popular due to their
revolutionary success in diverse areas, such as computer vision, natural
language processing, and speech recognition. However, the decision-making
processes of these models are generally not interpretable to users. In various
domains, such as healthcare, finance, or law, it is critical to know the
reasons behind a decision made by an artificial intelligence system. Therefore,
several directions for explaining neural models have recently been explored. In
this thesis, I investigate two major directions for explaining deep neural
networks. The first direction consists of feature-based post-hoc explanatory
methods, that is, methods that aim to explain an already trained and fixed
model (post-hoc), and that provide explanations in terms of input features,
such as tokens for text and superpixels for images (feature-based). The second
direction consists of self-explanatory neural models that generate natural
language explanations, that is, models that have a built-in module that
generates explanations for the predictions of the model.
",7.333333333333333,0.7394074,original,6.8,7.333333333333334,4
What are the latest techniques for explainability in deep neural networks?,"What are the latest techniques for explainability in deep neural networks? - Interpretability
- Model transparency
- Feature importance
- Explainable AI
- XAI techniques
- White-box models
- Black-box models
- Post-hoc explainability
- Local interpretability
- Global interpretability",Towards a Unified Evaluation of Explanation Methods without Ground Truth,"  This paper proposes a set of criteria to evaluate the objectiveness of
explanation methods of neural networks, which is crucial for the development of
explainable AI, but it also presents significant challenges. The core challenge
is that people usually cannot obtain ground-truth explanations of the neural
network. To this end, we design four metrics to evaluate explanation results
without ground-truth explanations. Our metrics can be broadly applied to nine
benchmark methods of interpreting neural networks, which provides new insights
of explanation methods.
",6.333333333333333,0.7702805,original,6.8,7.333333333333334,4
What are the latest techniques for explainability in deep neural networks?,"What are the latest techniques for explainability in deep neural networks? - Interpretability
- Model transparency
- Feature importance
- Explainable AI
- XAI techniques
- White-box models
- Black-box models
- Post-hoc explainability
- Local interpretability
- Global interpretability","How Good is your Explanation? Algorithmic Stability Measures to Assess
  the Quality of Explanations for Deep Neural Networks","  A plethora of methods have been proposed to explain how deep neural networks
reach their decisions but comparatively, little effort has been made to ensure
that the explanations produced by these methods are objectively relevant. While
several desirable properties for trustworthy explanations have been formulated,
objective measures have been harder to derive. Here, we propose two new
measures to evaluate explanations borrowed from the field of algorithmic
stability: mean generalizability MeGe and relative consistency ReCo. We conduct
extensive experiments on different network architectures, common explainability
methods, and several image datasets to demonstrate the benefits of the proposed
measures.In comparison to ours, popular fidelity measures are not sufficient to
guarantee trustworthy explanations.Finally, we found that 1-Lipschitz networks
produce explanations with higher MeGe and ReCo than common neural networks
while reaching similar accuracy. This suggests that 1-Lipschitz networks are a
relevant direction towards predictors that are more explainable and
trustworthy.
",7.0,0.7131523,original,6.8,7.333333333333334,4
What are the latest techniques for explainability in deep neural networks?,"What are the latest techniques for explainability in deep neural networks? - Interpretability
- Model transparency
- Feature importance
- Explainable AI
- XAI techniques
- White-box models
- Black-box models
- Post-hoc explainability
- Local interpretability
- Global interpretability",Gradient based Feature Attribution in Explainable AI: A Technical Review,"  The surge in black-box AI models has prompted the need to explain the
internal mechanism and justify their reliability, especially in high-stakes
applications, such as healthcare and autonomous driving. Due to the lack of a
rigorous definition of explainable AI (XAI), a plethora of research related to
explainability, interpretability, and transparency has been developed to
explain and analyze the model from various perspectives. Consequently, with an
exhaustive list of papers, it becomes challenging to have a comprehensive
overview of XAI research from all aspects. Considering the popularity of neural
networks in AI research, we narrow our focus to a specific area of XAI
research: gradient based explanations, which can be directly adopted for neural
network models. In this review, we systematically explore gradient based
explanation methods to date and introduce a novel taxonomy to categorize them
into four distinct classes. Then, we present the essence of technique details
in chronological order and underscore the evolution of algorithms. Next, we
introduce both human and quantitative evaluations to measure algorithm
performance. More importantly, we demonstrate the general challenges in XAI and
specific challenges in gradient based explanations. We hope that this survey
can help researchers understand state-of-the-art progress and their
corresponding disadvantages, which could spark their interest in addressing
these issues in future work.
",9.0,0.7168599,expanded,6.8,7.333333333333334,4
What are the latest techniques for explainability in deep neural networks?,"What are the latest techniques for explainability in deep neural networks? - Interpretability
- Model transparency
- Feature importance
- Explainable AI
- XAI techniques
- White-box models
- Black-box models
- Post-hoc explainability
- Local interpretability
- Global interpretability","Do Explanations Reflect Decisions? A Machine-centric Strategy to
  Quantify the Performance of Explainability Algorithms","  There has been a significant surge of interest recently around the concept of
explainable artificial intelligence (XAI), where the goal is to produce an
interpretation for a decision made by a machine learning algorithm. Of
particular interest is the interpretation of how deep neural networks make
decisions, given the complexity and `black box' nature of such networks. Given
the infancy of the field, there has been very limited exploration into the
assessment of the performance of explainability methods, with most evaluations
centered around subjective visual interpretation of the produced
interpretations. In this study, we explore a more machine-centric strategy for
quantifying the performance of explainability methods on deep neural networks
via the notion of decision-making impact analysis. We introduce two
quantitative performance metrics: i) Impact Score, which assesses the
percentage of critical factors with either strong confidence reduction impact
or decision changing impact, and ii) Impact Coverage, which assesses the
percentage coverage of adversarially impacted factors in the input. A
comprehensive analysis using this approach was conducted on several
state-of-the-art explainability methods (LIME, SHAP, Expected Gradients,
GSInquire) on a ResNet-50 deep convolutional neural network using a subset of
ImageNet for the task of image classification. Experimental results show that
the critical regions identified by LIME within the tested images had the lowest
impact on the decision-making process of the network (~38%), with progressive
increase in decision-making impact for SHAP (~44%), Expected Gradients (~51%),
and GSInquire (~76%). While by no means perfect, the hope is that the proposed
machine-centric strategy helps push the conversation forward towards better
metrics for evaluating explainability methods and improve trust in deep neural
networks.
",6.333333333333333,0.70346236,expanded,6.8,7.333333333333334,4
What are the latest techniques for explainability in deep neural networks?,"What are the latest techniques for explainability in deep neural networks? - Interpretability
- Model transparency
- Feature importance
- Explainable AI
- XAI techniques
- White-box models
- Black-box models
- Post-hoc explainability
- Local interpretability
- Global interpretability","Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
  Opportunities and Challenges toward Responsible AI","  In the last years, Artificial Intelligence (AI) has achieved a notable
momentum that may deliver the best of expectations over many application
sectors across the field. For this to occur, the entire community stands in
front of the barrier of explainability, an inherent problem of AI techniques
brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not
present in the last hype of AI. Paradigms underlying this problem fall within
the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial
feature for the practical deployment of AI models. This overview examines the
existing literature in the field of XAI, including a prospect toward what is
yet to be reached. We summarize previous efforts to define explainability in
Machine Learning, establishing a novel definition that covers prior conceptual
propositions with a major focus on the audience for which explainability is
sought. We then propose and discuss about a taxonomy of recent contributions
related to the explainability of different Machine Learning models, including
those aimed at Deep Learning methods for which a second taxonomy is built. This
literature analysis serves as the background for a series of challenges faced
by XAI, such as the crossroads between data fusion and explainability. Our
prospects lead toward the concept of Responsible Artificial Intelligence,
namely, a methodology for the large-scale implementation of AI methods in real
organizations with fairness, model explainability and accountability at its
core. Our ultimate goal is to provide newcomers to XAI with a reference
material in order to stimulate future research advances, but also to encourage
experts and professionals from other disciplines to embrace the benefits of AI
in their activity sectors, without any prior bias for its lack of
interpretability.
",6.666666666666667,0.67385924,expanded,6.8,7.333333333333334,4
How is transfer learning being applied in medical imaging analysis?,"How is transfer learning being applied in medical imaging analysis? - Deep learning
- Convolutional neural networks
- Radiology
- Disease diagnosis
- Feature extraction
- Image classification
- Image segmentation","What Makes Transfer Learning Work For Medical Images: Feature Reuse &
  Other Factors","  Transfer learning is a standard technique to transfer knowledge from one
domain to another. For applications in medical imaging, transfer from ImageNet
has become the de-facto approach, despite differences in the tasks and image
characteristics between the domains. However, it is unclear what factors
determine whether - and to what extent - transfer learning to the medical
domain is useful. The long-standing assumption that features from the source
domain get reused has recently been called into question. Through a series of
experiments on several medical image benchmark datasets, we explore the
relationship between transfer learning, data size, the capacity and inductive
bias of the model, as well as the distance between the source and target
domain. Our findings suggest that transfer learning is beneficial in most
cases, and we characterize the important role feature reuse plays in its
success.
",9.333333333333334,0.7203219,overlap,6.8,7.333333333333334,4
How is transfer learning being applied in medical imaging analysis?,"How is transfer learning being applied in medical imaging analysis? - Deep learning
- Convolutional neural networks
- Radiology
- Disease diagnosis
- Feature extraction
- Image classification
- Image segmentation",Supervised Transfer Learning at Scale for Medical Imaging,"  Transfer learning is a standard technique to improve performance on tasks
with limited data. However, for medical imaging, the value of transfer learning
is less clear. This is likely due to the large domain mismatch between the
usual natural-image pre-training (e.g. ImageNet) and medical images. However,
recent advances in transfer learning have shown substantial improvements from
scale. We investigate whether modern methods can change the fortune of transfer
learning for medical imaging. For this, we study the class of large-scale
pre-trained networks presented by Kolesnikov et al. on three diverse imaging
tasks: chest radiography, mammography, and dermatology. We study both transfer
performance and critical properties for the deployment in the medical domain,
including: out-of-distribution generalization, data-efficiency, sub-group
fairness, and uncertainty estimation. Interestingly, we find that for some of
these properties transfer from natural to medical images is indeed extremely
effective, but only when performed at sufficient scale.
",9.0,0.68376684,original,6.8,7.333333333333334,4
How is transfer learning being applied in medical imaging analysis?,"How is transfer learning being applied in medical imaging analysis? - Deep learning
- Convolutional neural networks
- Radiology
- Disease diagnosis
- Feature extraction
- Image classification
- Image segmentation",Modality-bridge Transfer Learning for Medical Image Classification,"  This paper presents a new approach of transfer learning-based medical image
classification to mitigate insufficient labeled data problem in medical domain.
Instead of direct transfer learning from source to small number of labeled
target data, we propose a modality-bridge transfer learning which employs the
bridge database in the same medical imaging acquisition modality as target
database. By learning the projection function from source to bridge and from
bridge to target, the domain difference between source (e.g., natural images)
and target (e.g., X-ray images) can be mitigated. Experimental results show
that the proposed method can achieve a high classification performance even for
a small number of labeled target medical images, compared to various transfer
learning approaches.
",9.0,0.69405055,original,6.8,7.333333333333334,4
How is transfer learning being applied in medical imaging analysis?,"How is transfer learning being applied in medical imaging analysis? - Deep learning
- Convolutional neural networks
- Radiology
- Disease diagnosis
- Feature extraction
- Image classification
- Image segmentation",Visual Transfer Learning: Informal Introduction and Literature Overview,"  Transfer learning techniques are important to handle small training sets and
to allow for quick generalization even from only a few examples. The following
paper is the introduction as well as the literature overview part of my thesis
related to the topic of transfer learning for visual recognition problems.
",3.3333333333333335,0.63325167,original,6.8,7.333333333333334,4
How is transfer learning being applied in medical imaging analysis?,"How is transfer learning being applied in medical imaging analysis? - Deep learning
- Convolutional neural networks
- Radiology
- Disease diagnosis
- Feature extraction
- Image classification
- Image segmentation","Exploring connections of spectral analysis and transfer learning in
  medical imaging","  In this paper, we use spectral analysis to investigate transfer learning and
study model sensitivity to frequency shortcuts in medical imaging. By analyzing
the power spectrum density of both pre-trained and fine-tuned model gradients,
as well as artificially generated frequency shortcuts, we observe notable
differences in learning priorities between models pre-trained on natural vs
medical images, which generally persist during fine-tuning. We find that when a
model's learning priority aligns with the power spectrum density of an
artifact, it results in overfitting to that artifact. Based on these
observations, we show that source data editing can alter the model's resistance
to shortcut learning.
",7.666666666666667,0.60416836,original,6.8,7.333333333333334,4
How is transfer learning being applied in medical imaging analysis?,"How is transfer learning being applied in medical imaging analysis? - Deep learning
- Convolutional neural networks
- Radiology
- Disease diagnosis
- Feature extraction
- Image classification
- Image segmentation","Towards Automated Melanoma Screening: Exploring Transfer Learning
  Schemes","  Deep learning is the current bet for image classification. Its greed for huge
amounts of annotated data limits its usage in medical imaging context. In this
scenario transfer learning appears as a prominent solution. In this report we
aim to clarify how transfer learning schemes may influence classification
results. We are particularly focused in the automated melanoma screening
problem, a case of medical imaging in which transfer learning is still not
widely used. We explored transfer with and without fine-tuning, sequential
transfers and usage of pre-trained models in general and specific datasets.
Although some issues remain open, our findings may drive future researches.
",8.666666666666666,0.6264371,expanded,6.8,7.333333333333334,4
How is transfer learning being applied in medical imaging analysis?,"How is transfer learning being applied in medical imaging analysis? - Deep learning
- Convolutional neural networks
- Radiology
- Disease diagnosis
- Feature extraction
- Image classification
- Image segmentation","Deep transfer learning for detecting Covid-19, Pneumonia and
  Tuberculosis using CXR images -- A Review","  Chest X-rays remains to be the most common imaging modality used to diagnose
lung diseases. However, they necessitate the interpretation of experts
(radiologists and pulmonologists), who are few. This review paper investigates
the use of deep transfer learning techniques to detect COVID-19, pneumonia, and
tuberculosis in chest X-ray (CXR) images. It provides an overview of current
state-of-the-art CXR image classification techniques and discusses the
challenges and opportunities in applying transfer learning to this domain. The
paper provides a thorough examination of recent research studies that used deep
transfer learning algorithms for COVID-19, pneumonia, and tuberculosis
detection, highlighting the advantages and disadvantages of these approaches.
Finally, the review paper discusses future research directions in the field of
deep transfer learning for CXR image classification, as well as the potential
for these techniques to aid in the diagnosis and treatment of lung diseases.
",9.333333333333334,0.6271579,expanded,6.8,7.333333333333334,4
How is transfer learning being applied in medical imaging analysis?,"How is transfer learning being applied in medical imaging analysis? - Deep learning
- Convolutional neural networks
- Radiology
- Disease diagnosis
- Feature extraction
- Image classification
- Image segmentation",Transfusion: Understanding Transfer Learning for Medical Imaging,"  Transfer learning from natural image datasets, particularly ImageNet, using
standard large models and corresponding pretrained weights has become a
de-facto method for deep learning applications to medical imaging. However,
there are fundamental differences in data sizes, features and task
specifications between natural image classification and the target medical
tasks, and there is little understanding of the effects of transfer. In this
paper, we explore properties of transfer learning for medical imaging. A
performance evaluation on two large scale medical imaging tasks shows that
surprisingly, transfer offers little benefit to performance, and simple,
lightweight models can perform comparably to ImageNet architectures.
Investigating the learned representations and features, we find that some of
the differences from transfer learning are due to the over-parametrization of
standard models rather than sophisticated feature reuse. We isolate where
useful feature reuse occurs, and outline the implications for more efficient
model exploration. We also explore feature independent benefits of transfer
arising from weight scalings.
",8.333333333333334,0.5879437,expanded,6.8,7.333333333333334,4
How is transfer learning being applied in medical imaging analysis?,"How is transfer learning being applied in medical imaging analysis? - Deep learning
- Convolutional neural networks
- Radiology
- Disease diagnosis
- Feature extraction
- Image classification
- Image segmentation","MedNet: Pre-trained Convolutional Neural Network Model for the Medical
  Imaging Tasks","  Deep Learning (DL) requires a large amount of training data to provide
quality outcomes. However, the field of medical imaging suffers from the lack
of sufficient data for properly training DL models because medical images
require manual labelling carried out by clinical experts thus the process is
time-consuming, expensive, and error-prone. Recently, transfer learning (TL)
was introduced to reduce the need for the annotation procedure by means of
transferring the knowledge performed by a previous task and then fine-tuning
the result using a relatively small dataset. Nowadays, multiple classification
methods from medical imaging make use of TL from general-purpose pre-trained
models, e.g., ImageNet, which has been proven to be ineffective due to the
mismatch between the features learned from natural images (ImageNet) and those
more specific from medical images especially medical gray images such as
X-rays. ImageNet does not have grayscale images such as MRI, CT, and X-ray. In
this paper, we propose a novel DL model to be used for addressing
classification tasks of medical imaging, called MedNet. To do so, we aim to
issue two versions of MedNet. The first one is Gray-MedNet which will be
trained on 3M publicly available gray-scale medical images including MRI, CT,
X-ray, ultrasound, and PET. The second version is Color-MedNet which will be
trained on 3M publicly available color medical images including histopathology,
taken images, and many others. To validate the effectiveness MedNet, both
versions will be fine-tuned to train on the target tasks of a more reduced set
of medical images. MedNet performs as the pre-trained model to tackle any
real-world application from medical imaging and achieve the level of
generalization needed for dealing with medical imaging tasks, e.g.
classification. MedNet would serve the research community as a baseline for
future research.
",9.333333333333334,0.5786597,expanded,6.8,7.333333333333334,4
What are the advancements in handling class imbalance in supervised learning?,"What are the advancements in handling class imbalance in supervised learning? - Sampling techniques
- Synthetic data generation
- Cost-sensitive learning
- Ensemble methods
- Evaluation metrics for imbalanced datasets","Review of Methods for Handling Class-Imbalanced in Classification
  Problems","  Learning classifiers using skewed or imbalanced datasets can occasionally
lead to classification issues; this is a serious issue. In some cases, one
class contains the majority of examples while the other, which is frequently
the more important class, is nevertheless represented by a smaller proportion
of examples. Using this kind of data could make many carefully designed
machine-learning systems ineffective. High training fidelity was a term used to
describe biases vs. all other instances of the class. The best approach to all
possible remedies to this issue is typically to gain from the minority class.
The article examines the most widely used methods for addressing the problem of
learning with a class imbalance, including data-level, algorithm-level, hybrid,
cost-sensitive learning, and deep learning, etc. including their advantages and
limitations. The efficiency and performance of the classifier are assessed
using a myriad of evaluation metrics.
",9.0,0.70557666,overlap,6.8,7.333333333333334,4
What are the advancements in handling class imbalance in supervised learning?,"What are the advancements in handling class imbalance in supervised learning? - Sampling techniques
- Synthetic data generation
- Cost-sensitive learning
- Ensemble methods
- Evaluation metrics for imbalanced datasets",Rethinking Class Imbalance in Machine Learning,"  Imbalance learning is a subfield of machine learning that focuses on learning
tasks in the presence of class imbalance. Nearly all existing studies refer to
class imbalance as a proportion imbalance, where the proportion of training
samples in each class is not balanced. The ignorance of the proportion
imbalance will result in unfairness between/among classes and poor
generalization capability. Previous literature has presented numerous methods
for either theoretical/empirical analysis or new methods for imbalance
learning. This study presents a new taxonomy of class imbalance in machine
learning with a broader scope. Four other types of imbalance, namely, variance,
distance, neighborhood, and quality imbalances between/among classes, which may
exist in machine learning tasks, are summarized. Two different levels of
imbalance including global and local are also presented. Theoretical analysis
is used to illustrate the significant impact of the new imbalance types on
learning fairness. Moreover, our taxonomy and theoretical conclusions are used
to analyze the shortcomings of several classical methods. As an example, we
propose a new logit perturbation-based imbalance learning loss when proportion,
variance, and distance imbalances exist simultaneously. Several classical
losses become the special case of our proposed method. Meta learning is
utilized to infer the hyper-parameters related to the three types of imbalance.
Experimental results on several benchmark corpora validate the effectiveness of
the proposed method.
",8.666666666666666,0.7197063,original,6.8,7.333333333333334,4
What are the advancements in handling class imbalance in supervised learning?,"What are the advancements in handling class imbalance in supervised learning? - Sampling techniques
- Synthetic data generation
- Cost-sensitive learning
- Ensemble methods
- Evaluation metrics for imbalanced datasets",Influence-Balanced Loss for Imbalanced Visual Classification,"  In this paper, we propose a balancing training method to address problems in
imbalanced data learning. To this end, we derive a new loss used in the
balancing training phase that alleviates the influence of samples that cause an
overfitted decision boundary. The proposed loss efficiently improves the
performance of any type of imbalance learning methods. In experiments on
multiple benchmark data sets, we demonstrate the validity of our method and
reveal that the proposed loss outperforms the state-of-the-art cost-sensitive
loss methods. Furthermore, since our loss is not restricted to a specific task,
model, or training method, it can be easily used in combination with other
recent re-sampling, meta-learning, and cost-sensitive learning methods for
class-imbalance problems.
",8.333333333333334,0.70390683,overlap,6.8,7.333333333333334,4
What are the advancements in handling class imbalance in supervised learning?,"What are the advancements in handling class imbalance in supervised learning? - Sampling techniques
- Synthetic data generation
- Cost-sensitive learning
- Ensemble methods
- Evaluation metrics for imbalanced datasets","An Empirical Analysis of the Efficacy of Different Sampling Techniques
  for Imbalanced Classification","  Learning from imbalanced data is a challenging task. Standard classification
algorithms tend to perform poorly when trained on imbalanced data. Some special
strategies need to be adopted, either by modifying the data distribution or by
redesigning the underlying classification algorithm to achieve desirable
performance. The prevalence of imbalance in real-world datasets has led to the
creation of a multitude of strategies for the class imbalance issue. However,
not all the strategies are useful or provide good performance in different
imbalance scenarios. There are numerous approaches to dealing with imbalanced
data, but the efficacy of such techniques or an experimental comparison among
those techniques has not been conducted. In this study, we present a
comprehensive analysis of 26 popular sampling techniques to understand their
effectiveness in dealing with imbalanced data. Rigorous experiments have been
conducted on 50 datasets with different degrees of imbalance to thoroughly
investigate the performance of these techniques. A detailed discussion of the
advantages and limitations of the techniques, as well as how to overcome such
limitations, has been presented. We identify some critical factors that affect
the sampling strategies and provide recommendations on how to choose an
appropriate sampling technique for a particular application.
",9.333333333333334,0.679003,overlap,6.8,7.333333333333334,4
What are the advancements in handling class imbalance in supervised learning?,"What are the advancements in handling class imbalance in supervised learning? - Sampling techniques
- Synthetic data generation
- Cost-sensitive learning
- Ensemble methods
- Evaluation metrics for imbalanced datasets",On Model Evaluation under Non-constant Class Imbalance,"  Many real-world classification problems are significantly class-imbalanced to
detriment of the class of interest. The standard set of proper evaluation
metrics is well-known but the usual assumption is that the test dataset
imbalance equals the real-world imbalance. In practice, this assumption is
often broken for various reasons. The reported results are then often too
optimistic and may lead to wrong conclusions about industrial impact and
suitability of proposed techniques. We introduce methods focusing on evaluation
under non-constant class imbalance. We show that not only the absolute values
of commonly used metrics, but even the order of classifiers in relation to the
evaluation metric used is affected by the change of the imbalance rate.
Finally, we demonstrate that using subsampling in order to get a test dataset
with class imbalance equal to the one observed in the wild is not necessary,
and eventually can lead to significant errors in classifier's performance
estimate.
",6.666666666666667,0.692309,original,6.8,7.333333333333334,4
What are the advancements in handling class imbalance in supervised learning?,"What are the advancements in handling class imbalance in supervised learning? - Sampling techniques
- Synthetic data generation
- Cost-sensitive learning
- Ensemble methods
- Evaluation metrics for imbalanced datasets","A Survey of Methods for Managing the Classification and Solution of Data
  Imbalance Problem","  The problem of class imbalance is extensive for focusing on numerous
applications in the real world. In such a situation, nearly all of the examples
are labeled as one class called majority class, while far fewer examples are
labeled as the other class usually, the more important class is called
minority. Over the last few years, several types of research have been carried
out on the issue of class imbalance, including data sampling, cost-sensitive
analysis, Genetic Programming based models, bagging, boosting, etc.
Nevertheless, in this survey paper, we enlisted the 24 related studies in the
years 2003, 2008, 2010, 2012 and 2014 to 2019, focusing on the architecture of
single, hybrid, and ensemble method design to understand the current status of
improving classification output in machine learning techniques to fix problems
with class imbalances. This survey paper also includes a statistical analysis
of the classification algorithms under various methods and several other
experimental conditions, as well as datasets used in different research papers.
",8.333333333333334,0.66323924,expanded,6.8,7.333333333333334,4
What are the advancements in handling class imbalance in supervised learning?,"What are the advancements in handling class imbalance in supervised learning? - Sampling techniques
- Synthetic data generation
- Cost-sensitive learning
- Ensemble methods
- Evaluation metrics for imbalanced datasets","An empirical evaluation of imbalanced data strategies from a
  practitioner's point of view","  This paper evaluates six strategies for mitigating imbalanced data:
oversampling, undersampling, ensemble methods, specialized algorithms, class
weight adjustments, and a no-mitigation approach referred to as the baseline.
These strategies were tested on 58 real-life binary imbalanced datasets with
imbalance rates ranging from 3 to 120. We conducted a comparative analysis of
10 under-sampling algorithms, 5 over-sampling algorithms, 2 ensemble methods,
and 3 specialized algorithms across eight different performance metrics:
accuracy, area under the ROC curve (AUC), balanced accuracy, F1-measure,
G-mean, Matthew's correlation coefficient, precision, and recall. Additionally,
we assessed the six strategies on altered datasets, derived from real-life
data, with both low (3) and high (100 or 300) imbalance ratios (IR).
  The principal finding indicates that the effectiveness of each strategy
significantly varies depending on the metric used. The paper also examines a
selection of newer algorithms within the categories of specialized algorithms,
oversampling, and ensemble methods. The findings suggest that the current
hierarchy of best-performing strategies for each metric is unlikely to change
with the introduction of newer algorithms.
",8.666666666666666,0.63253766,expanded,6.8,7.333333333333334,4
How can reinforcement learning improve autonomous vehicle navigation?,"How can reinforcement learning improve autonomous vehicle navigation? - Machine learning algorithms
- Deep reinforcement learning
- Training models
- Decision-making process
- Real-time navigation
- Sensor data
- Risk assessment 
- Safety measures
- Simulated environments","Holistic Deep-Reinforcement-Learning-based Training of Autonomous
  Navigation Systems","  In recent years, Deep Reinforcement Learning emerged as a promising approach
for autonomous navigation of ground vehicles and has been utilized in various
areas of navigation such as cruise control, lane changing, or obstacle
avoidance. However, most research works either focus on providing an end-to-end
solution training the whole system using Deep Reinforcement Learning or focus
on one specific aspect such as local motion planning. This however, comes along
with a number of problems such as catastrophic forgetfulness, inefficient
navigation behavior, and non-optimal synchronization between different entities
of the navigation stack. In this paper, we propose a holistic Deep
Reinforcement Learning training approach in which the training procedure is
involving all entities of the navigation stack. This should enhance the
synchronization between- and understanding of all entities of the navigation
stack and as a result, improve navigational performance. We trained several
agents with a number of different observation spaces to study the impact of
different input on the navigation behavior of the agent. In profound
evaluations against multiple learning-based and classic model-based navigation
approaches, our proposed agent could outperform the baselines in terms of
efficiency and safety attaining shorter path lengths, less roundabout paths,
and less collisions.
",9.0,0.73786813,overlap,6.8,7.333333333333334,4
How can reinforcement learning improve autonomous vehicle navigation?,"How can reinforcement learning improve autonomous vehicle navigation? - Machine learning algorithms
- Deep reinforcement learning
- Training models
- Decision-making process
- Real-time navigation
- Sensor data
- Risk assessment 
- Safety measures
- Simulated environments",Benchmarking Reinforcement Learning Techniques for Autonomous Navigation,"  Deep reinforcement learning (RL) has brought many successes for autonomous
robot navigation. However, there still exists important limitations that
prevent real-world use of RL-based navigation systems. For example, most
learning approaches lack safety guarantees; and learned navigation systems may
not generalize well to unseen environments. Despite a variety of recent
learning techniques to tackle these challenges in general, a lack of an
open-source benchmark and reproducible learning methods specifically for
autonomous navigation makes it difficult for roboticists to choose what
learning methods to use for their mobile robots and for learning researchers to
identify current shortcomings of general learning methods for autonomous
navigation. In this paper, we identify four major desiderata of applying deep
RL approaches for autonomous navigation: (D1) reasoning under uncertainty, (D2)
safety, (D3) learning from limited trial-and-error data, and (D4)
generalization to diverse and novel environments. Then, we explore four major
classes of learning techniques with the purpose of achieving one or more of the
four desiderata: memory-based neural network architectures (D1), safe RL (D2),
model-based RL (D2, D3), and domain randomization (D4). By deploying these
learning techniques in a new open-source large-scale navigation benchmark and
real-world environments, we perform a comprehensive study aimed at establishing
to what extent can these techniques achieve these desiderata for RL-based
navigation systems.
",9.333333333333334,0.70179206,original,6.8,7.333333333333334,4
How can reinforcement learning improve autonomous vehicle navigation?,"How can reinforcement learning improve autonomous vehicle navigation? - Machine learning algorithms
- Deep reinforcement learning
- Training models
- Decision-making process
- Real-time navigation
- Sensor data
- Risk assessment 
- Safety measures
- Simulated environments","Evaluation of Safety Constraints in Autonomous Navigation with Deep
  Reinforcement Learning","  While reinforcement learning algorithms have had great success in the field
of autonomous navigation, they cannot be straightforwardly applied to the real
autonomous systems without considering the safety constraints. The later are
crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To
highlight the importance of these constraints, in this study, we compare two
learnable navigation policies: safe and unsafe. The safe policy takes the
constraints into account, while the other does not. We show that the safe
policy is able to generate trajectories with more clearance (distance to the
obstacles) and makes less collisions while training without sacrificing the
overall performance.
",8.333333333333334,0.733965,overlap,6.8,7.333333333333334,4
How can reinforcement learning improve autonomous vehicle navigation?,"How can reinforcement learning improve autonomous vehicle navigation? - Machine learning algorithms
- Deep reinforcement learning
- Training models
- Decision-making process
- Real-time navigation
- Sensor data
- Risk assessment 
- Safety measures
- Simulated environments","A Survey of Deep Reinforcement Learning Algorithms for Motion Planning
  and Control of Autonomous Vehicles","  In this survey, we systematically summarize the current literature on studies
that apply reinforcement learning (RL) to the motion planning and control of
autonomous vehicles. Many existing contributions can be attributed to the
pipeline approach, which consists of many hand-crafted modules, each with a
functionality selected for the ease of human interpretation. However, this
approach does not automatically guarantee maximal performance due to the lack
of a system-level optimization. Therefore, this paper also presents a growing
trend of work that falls into the end-to-end approach, which typically offers
better performance and smaller system scales. However, their performance also
suffers from the lack of expert data and generalization issues. Finally, the
remaining challenges applying deep RL algorithms on autonomous driving are
summarized, and future research directions are also presented to tackle these
challenges.
",9.0,0.6954729,original,6.8,7.333333333333334,4
How can reinforcement learning improve autonomous vehicle navigation?,"How can reinforcement learning improve autonomous vehicle navigation? - Machine learning algorithms
- Deep reinforcement learning
- Training models
- Decision-making process
- Real-time navigation
- Sensor data
- Risk assessment 
- Safety measures
- Simulated environments",Learning to Drive in a Day,"  We demonstrate the first application of deep reinforcement learning to
autonomous driving. From randomly initialised parameters, our model is able to
learn a policy for lane following in a handful of training episodes using a
single monocular image as input. We provide a general and easy to obtain
reward: the distance travelled by the vehicle without the safety driver taking
control. We use a continuous, model-free deep reinforcement learning algorithm,
with all exploration and optimisation performed on-vehicle. This demonstrates a
new framework for autonomous driving which moves away from reliance on defined
logical rules, mapping, and direct supervision. We discuss the challenges and
opportunities to scale this approach to a broader range of autonomous driving
tasks.
",8.0,0.6856755,original,6.8,7.333333333333334,4
How can reinforcement learning improve autonomous vehicle navigation?,"How can reinforcement learning improve autonomous vehicle navigation? - Machine learning algorithms
- Deep reinforcement learning
- Training models
- Decision-making process
- Real-time navigation
- Sensor data
- Risk assessment 
- Safety measures
- Simulated environments","Reinforcement Learning Based Safe Decision Making for Highway Autonomous
  Driving","  In this paper, we develop a safe decision-making method for self-driving cars
in a multi-lane, single-agent setting. The proposed approach utilizes deep
reinforcement learning (RL) to achieve a high-level policy for safe tactical
decision-making. We address two major challenges that arise solely in
autonomous navigation. First, the proposed algorithm ensures that collisions
never happen, and therefore accelerate the learning process. Second, the
proposed algorithm takes into account the unobservable states in the
environment. These states appear mainly due to the unpredictable behavior of
other agents, such as cars, and pedestrians, and make the Markov Decision
Process (MDP) problematic when dealing with autonomous navigation. Simulations
from a well-known self-driving car simulator demonstrate the applicability of
the proposed method
",9.333333333333334,0.68416333,expanded,6.8,7.333333333333334,4
How can reinforcement learning improve autonomous vehicle navigation?,"How can reinforcement learning improve autonomous vehicle navigation? - Machine learning algorithms
- Deep reinforcement learning
- Training models
- Decision-making process
- Real-time navigation
- Sensor data
- Risk assessment 
- Safety measures
- Simulated environments",End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing,"  Reinforcement Learning (RL) has emerged as a transformative approach in the
domains of automation and robotics, offering powerful solutions to complex
problems that conventional methods struggle to address. In scenarios where the
problem definitions are elusive and challenging to quantify, learning-based
solutions such as RL become particularly valuable. One instance of such
complexity can be found in the realm of car racing, a dynamic and unpredictable
environment that demands sophisticated decision-making algorithms. This study
focuses on developing and training an RL agent to navigate a racing environment
solely using feedforward raw lidar and velocity data in a simulated context.
The agent's performance, trained in the simulation environment, is then
experimentally evaluated in a real-world racing scenario. This exploration
underlines the feasibility and potential benefits of RL algorithm enhancing
autonomous racing performance, especially in the environments where prior map
information is not available.
",9.0,0.71643203,expanded,6.8,7.333333333333334,4
How can reinforcement learning improve autonomous vehicle navigation?,"How can reinforcement learning improve autonomous vehicle navigation? - Machine learning algorithms
- Deep reinforcement learning
- Training models
- Decision-making process
- Real-time navigation
- Sensor data
- Risk assessment 
- Safety measures
- Simulated environments","Formula RL: Deep Reinforcement Learning for Autonomous Racing using
  Telemetry Data","  This paper explores the use of reinforcement learning (RL) models for
autonomous racing. In contrast to passenger cars, where safety is the top
priority, a racing car aims to minimize the lap-time. We frame the problem as a
reinforcement learning task with a multidimensional input consisting of the
vehicle telemetry, and a continuous action space. To find out which RL methods
better solve the problem and whether the obtained models generalize to driving
on unknown tracks, we put 10 variants of deep deterministic policy gradient
(DDPG) to race in two experiments: i)~studying how RL methods learn to drive a
racing car and ii)~studying how the learning scenario influences the capability
of the models to generalize. Our studies show that models trained with RL are
not only able to drive faster than the baseline open source handcrafted bots
but also generalize to unknown tracks.
",8.333333333333334,0.68290406,expanded,6.8,7.333333333333334,4
What methods exist for detecting outliers in financial time series?,"What methods exist for detecting outliers in financial time series? Machine learning, statistical methods, anomaly detection, clustering, deep learning, data preprocessing, time series analysis",A review on outlier/anomaly detection in time series data,"  Recent advances in technology have brought major breakthroughs in data
collection, enabling a large amount of data to be gathered over time and thus
generating time series. Mining this data has become an important task for
researchers and practitioners in the past few years, including the detection of
outliers or anomalies that may represent errors or events of interest. This
review aims to provide a structured and comprehensive state-of-the-art on
outlier detection techniques in the context of time series. To this end, a
taxonomy is presented based on the main aspects that characterize an outlier
detection technique.
",8.666666666666666,0.7724327,overlap,6.8,7.333333333333334,4
What methods exist for detecting outliers in financial time series?,"What methods exist for detecting outliers in financial time series? Machine learning, statistical methods, anomaly detection, clustering, deep learning, data preprocessing, time series analysis",Outliers in dynamic factor models,"  Dynamic factor models have a wide range of applications in econometrics and
applied economics. The basic motivation resides in their capability of reducing
a large set of time series to only few indicators (factors). If the number of
time series is large compared to the available number of observations then most
information may be conveyed to the factors. This way low dimension models may
be estimated for explaining and forecasting one or more time series of
interest. It is desirable that outlier free time series be available for
estimation. In practice, outlying observations are likely to arise at unknown
dates due, for instance, to external unusual events or gross data entry errors.
Several methods for outlier detection in time series are available. Most
methods, however, apply to univariate time series while even methods designed
for handling the multivariate framework do not include dynamic factor models
explicitly. A method for discovering outliers occurrences in a dynamic factor
model is introduced that is based on linear transforms of the observed data.
Some strategies to separate outliers that add to the model and outliers within
the common component are discussed. Applications to simulated and real data
sets are presented to check the effectiveness of the proposed method.
",8.333333333333334,0.67511964,original,6.8,7.333333333333334,4
What methods exist for detecting outliers in financial time series?,"What methods exist for detecting outliers in financial time series? Machine learning, statistical methods, anomaly detection, clustering, deep learning, data preprocessing, time series analysis","Machine learning based forecasting of significant daily returns in
  foreign exchange markets","  Asset value forecasting has always attracted an enormous amount of interest
among researchers in quantitative analysis. The advent of modern machine
learning models has introduced new tools to tackle this classical problem. In
this paper, we apply machine learning algorithms to hitherto unexplored
question of forecasting instances of significant fluctuations in currency
exchange rates. We perform analysis of nine modern machine learning algorithms
using data on four major currency pairs over a 10 year period. A key
contribution is the novel use of outlier detection methods for this purpose.
Numerical experiments show that outlier detection methods substantially
outperform traditional machine learning and finance techniques. In addition, we
show that a recently proposed new outlier detection method PKDE produces best
overall results. Our findings hold across different currency pairs,
significance levels, and time horizons indicating the robustness of the
proposed method.
",8.666666666666666,0.6568352,overlap,6.8,7.333333333333334,4
What methods exist for detecting outliers in financial time series?,"What methods exist for detecting outliers in financial time series? Machine learning, statistical methods, anomaly detection, clustering, deep learning, data preprocessing, time series analysis","How to find a unicorn: a novel model-free, unsupervised anomaly
  detection method for time series","  Recognition of anomalous events is a challenging but critical task in many
scientific and industrial fields, especially when the properties of anomalies
are unknown. In this paper, we introduce a new anomaly concept called ""unicorn""
or unique event and present a new, model-free, unsupervised detection algorithm
to detect unicorns. The key component of the new algorithm is the Temporal
Outlier Factor (TOF) to measure the uniqueness of events in continuous data
sets from dynamic systems. The concept of unique events differs significantly
from traditional outliers in many aspects: while repetitive outliers are no
longer unique events, a unique event is not necessarily an outlier; it does not
necessarily fall out from the distribution of normal activity. The performance
of our algorithm was examined in recognizing unique events on different types
of simulated data sets with anomalies and it was compared with the Local
Outlier Factor (LOF) and discord discovery algorithms. TOF had superior
performance compared to LOF and discord algorithms even in recognizing
traditional outliers and it also recognized unique events that those did not.
The benefits of the unicorn concept and the new detection method were
illustrated by example data sets from very different scientific fields. Our
algorithm successfully recognized unique events in those cases where they were
already known such as the gravitational waves of a binary black hole merger on
LIGO detector data and the signs of respiratory failure on ECG data series.
Furthermore, unique events were found on the LIBOR data set of the last 30
years.
",6.666666666666667,0.54704696,original,6.8,7.333333333333334,4
What methods exist for detecting outliers in financial time series?,"What methods exist for detecting outliers in financial time series? Machine learning, statistical methods, anomaly detection, clustering, deep learning, data preprocessing, time series analysis",Feedforward Neural Network for Time Series Anomaly Detection,"  Time series anomaly detection is usually formulated as finding outlier data
points relative to some usual data, which is also an important problem in
industry and academia. To ensure systems working stably, internet companies,
banks and other companies need to monitor time series, which is called KPI (Key
Performance Indicators), such as CPU used, number of orders, number of online
users and so on. However, millions of time series have several shapes (e.g.
seasonal KPIs, KPIs of timed tasks and KPIs of CPU used), so that it is very
difficult to use a simple statistical model to detect anomaly for all kinds of
time series. Although some anomaly detectors have developed many years and some
supervised models are also available in this field, we find many methods have
their own disadvantages. In this paper, we present our system, which is based
on deep feedforward neural network and detect anomaly points of time series.
The main difference between our system and other systems based on supervised
models is that we do not need feature engineering of time series to train deep
feedforward neural network in our system, which is essentially an end-to-end
system.
",6.666666666666667,0.634165,overlap,6.8,7.333333333333334,4
What methods exist for detecting outliers in financial time series?,"What methods exist for detecting outliers in financial time series? Machine learning, statistical methods, anomaly detection, clustering, deep learning, data preprocessing, time series analysis",A Review of Open Source Software Tools for Time Series Analysis,"  Time series data is used in a wide range of real world applications. In a
variety of domains , detailed analysis of time series data (via Forecasting and
Anomaly Detection) leads to a better understanding of how events associated
with a specific time instance behave. Time Series Analysis (TSA) is commonly
performed with plots and traditional models. Machine Learning (ML) approaches ,
on the other hand , have seen an increase in the state of the art for
Forecasting and Anomaly Detection because they provide comparable results when
time and data constraints are met. A number of time series toolboxes are
available that offer rich interfaces to specific model classes (ARIMA/filters ,
neural networks) or framework interfaces to isolated time series modelling
tasks (forecasting , feature extraction , annotation , classification).
Nonetheless , open source machine learning capabilities for time series remain
limited , and existing libraries are frequently incompatible with one another.
The goal of this paper is to provide a concise and user friendly overview of
the most important open source tools for time series analysis. This article
examines two related toolboxes (1) forecasting and (2) anomaly detection. This
paper describes a typical Time Series Analysis (TSA) framework with an
architecture and lists the main features of TSA framework. The tools are
categorized based on the criteria of analysis tasks completed , data
preparation methods employed , and evaluation methods for results generated.
This paper presents quantitative analysis and discusses the current state of
actively developed open source Time Series Analysis frameworks. Overall , this
article considered 60 time series analysis tools , and 32 of which provided
forecasting modules , and 21 packages included anomaly detection.
",5.666666666666667,0.5484321,expanded,6.8,7.333333333333334,4
What methods exist for detecting outliers in financial time series?,"What methods exist for detecting outliers in financial time series? Machine learning, statistical methods, anomaly detection, clustering, deep learning, data preprocessing, time series analysis",An Evaluation of Classification and Outlier Detection Algorithms,"  This paper evaluates algorithms for classification and outlier detection
accuracies in temporal data. We focus on algorithms that train and classify
rapidly and can be used for systems that need to incorporate new data
regularly. Hence, we compare the accuracy of six fast algorithms using a range
of well-known time-series datasets. The analyses demonstrate that the choice of
algorithm is task and data specific but that we can derive heuristics for
choosing. Gradient Boosting Machines are generally best for classification but
there is no single winner for outlier detection though Gradient Boosting
Machines (again) and Random Forest are better. Hence, we recommend running
evaluations of a number of algorithms using our heuristics.
",4.666666666666667,0.6032147,expanded,6.8,7.333333333333334,4
What are recent innovations in synthetic data generation for NLP?,"What are recent innovations in synthetic data generation for NLP? - GANs
- Transformer models
- Autoencoders
- Privacy-preserving techniques
- Domain adaptation
- Transfer learning
- Self-supervised learning","On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A
  Survey","  Within the evolving landscape of deep learning, the dilemma of data quantity
and quality has been a long-standing problem. The recent advent of Large
Language Models (LLMs) offers a data-centric solution to alleviate the
limitations of real-world data with synthetic data generation. However, current
investigations into this field lack a unified framework and mostly stay on the
surface. Therefore, this paper provides an organization of relevant studies
based on a generic workflow of synthetic data generation. By doing so, we
highlight the gaps within existing research and outline prospective avenues for
future study. This work aims to shepherd the academic and industrial
communities towards deeper, more methodical inquiries into the capabilities and
applications of LLMs-driven synthetic data generation.
",8.666666666666666,0.7276604,overlap,6.8,7.333333333333334,4
What are recent innovations in synthetic data generation for NLP?,"What are recent innovations in synthetic data generation for NLP? - GANs
- Transformer models
- Autoencoders
- Privacy-preserving techniques
- Domain adaptation
- Transfer learning
- Self-supervised learning",Best Practices and Lessons Learned on Synthetic Data for Language Models,"  The success of AI models relies on the availability of large, diverse, and
high-quality datasets, which can be challenging to obtain due to data scarcity,
privacy concerns, and high costs. Synthetic data has emerged as a promising
solution by generating artificial data that mimics real-world patterns. This
paper provides an overview of synthetic data research, discussing its
applications, challenges, and future directions. We present empirical evidence
from prior art to demonstrate its effectiveness and highlight the importance of
ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for
responsible use of synthetic data to build more powerful, inclusive, and
trustworthy language models.
",6.666666666666667,0.69508004,overlap,6.8,7.333333333333334,4
What are recent innovations in synthetic data generation for NLP?,"What are recent innovations in synthetic data generation for NLP? - GANs
- Transformer models
- Autoencoders
- Privacy-preserving techniques
- Domain adaptation
- Transfer learning
- Self-supervised learning","Generative AI for Synthetic Data Generation: Methods, Challenges and the
  Future","  The recent surge in research focused on generating synthetic data from large
language models (LLMs), especially for scenarios with limited data
availability, marks a notable shift in Generative Artificial Intelligence (AI).
Their ability to perform comparably to real-world data positions this approach
as a compelling solution to low-resource challenges. This paper delves into
advanced technologies that leverage these gigantic LLMs for the generation of
task-specific training data. We outline methodologies, evaluation techniques,
and practical applications, discuss the current limitations, and suggest
potential pathways for future research.
",8.666666666666666,0.65776056,overlap,6.8,7.333333333333334,4
What are recent innovations in synthetic data generation for NLP?,"What are recent innovations in synthetic data generation for NLP? - GANs
- Transformer models
- Autoencoders
- Privacy-preserving techniques
- Domain adaptation
- Transfer learning
- Self-supervised learning","Shape of synth to come: Why we should use synthetic data for English
  surface realization","  The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language
Generation shared tasks with the goal of exploring approaches to surface
realization from Universal-Dependency-like trees to surface strings for several
languages. In the 2018 shared task there was very little difference in the
absolute performance of systems trained with and without additional,
synthetically created data, and a new rule prohibiting the use of synthetic
data was introduced for the 2019 shared task. Contrary to the findings of the
2018 shared task, we show, in experiments on the English 2018 dataset, that the
use of synthetic data can have a substantial positive effect - an improvement
of almost 8 BLEU points for a previously state-of-the-art system. We analyse
the effects of synthetic data, and we argue that its use should be encouraged
rather than prohibited so that future research efforts continue to explore
systems that can take advantage of such data.
",6.333333333333333,0.700649,original,6.8,7.333333333333334,4
What are recent innovations in synthetic data generation for NLP?,"What are recent innovations in synthetic data generation for NLP? - GANs
- Transformer models
- Autoencoders
- Privacy-preserving techniques
- Domain adaptation
- Transfer learning
- Self-supervised learning",Does Synthetic Data Make Large Language Models More Efficient?,"  Natural Language Processing (NLP) has undergone transformative changes with
the advent of deep learning methodologies. One challenge persistently
confronting researchers is the scarcity of high-quality, annotated datasets
that drive these models. This paper explores the nuances of synthetic data
generation in NLP, with a focal point on template-based question generation. By
assessing its advantages, including data augmentation potential and the
introduction of structured variety, we juxtapose these benefits against
inherent limitations, such as the risk of overfitting and the constraints posed
by pre-defined templates. Drawing from empirical evaluations, we demonstrate
the impact of template-based synthetic data on the performance of modern
transformer models. We conclude by emphasizing the delicate balance required
between synthetic and real-world data, and the future trajectories of
integrating synthetic data in model training pipelines. The findings aim to
guide NLP practitioners in harnessing synthetic data's potential, ensuring
optimal model performance in diverse applications.
",7.666666666666667,0.6357754,original,6.8,7.333333333333334,4
What are recent innovations in synthetic data generation for NLP?,"What are recent innovations in synthetic data generation for NLP? - GANs
- Transformer models
- Autoencoders
- Privacy-preserving techniques
- Domain adaptation
- Transfer learning
- Self-supervised learning",Machine Learning for Synthetic Data Generation: A Review,"  Machine learning heavily relies on data, but real-world applications often
encounter various data-related issues. These include data of poor quality,
insufficient data points leading to under-fitting of machine learning models,
and difficulties in data access due to concerns surrounding privacy, safety,
and regulations. In light of these challenges, the concept of synthetic data
generation emerges as a promising alternative that allows for data sharing and
utilization in ways that real-world data cannot facilitate. This paper presents
a comprehensive systematic review of existing studies that employ machine
learning models for the purpose of generating synthetic data. The review
encompasses various perspectives, starting with the applications of synthetic
data generation, spanning computer vision, speech, natural language processing,
healthcare, and business domains. Additionally, it explores different machine
learning methods, with particular emphasis on neural network architectures and
deep generative models. The paper also addresses the crucial aspects of privacy
and fairness concerns related to synthetic data generation. Furthermore, this
study identifies the challenges and opportunities prevalent in this emerging
field, shedding light on the potential avenues for future research. By delving
into the intricacies of synthetic data generation, this paper aims to
contribute to the advancement of knowledge and inspire further exploration in
synthetic data generation.
",6.333333333333333,0.625636,expanded,6.8,7.333333333333334,4
What are recent innovations in synthetic data generation for NLP?,"What are recent innovations in synthetic data generation for NLP? - GANs
- Transformer models
- Autoencoders
- Privacy-preserving techniques
- Domain adaptation
- Transfer learning
- Self-supervised learning","Beyond Privacy: Navigating the Opportunities and Challenges of Synthetic
  Data","  Generating synthetic data through generative models is gaining interest in
the ML community and beyond. In the past, synthetic data was often regarded as
a means to private data release, but a surge of recent papers explore how its
potential reaches much further than this -- from creating more fair data to
data augmentation, and from simulation to text generated by ChatGPT. In this
perspective we explore whether, and how, synthetic data may become a dominant
force in the machine learning world, promising a future where datasets can be
tailored to individual needs. Just as importantly, we discuss which fundamental
challenges the community needs to overcome for wider relevance and application
of synthetic data -- the most important of which is quantifying how much we can
trust any finding or prediction drawn from synthetic data.
",6.666666666666667,0.6623593,expanded,6.8,7.333333333333334,4
How is unsupervised learning being used in anomaly detection?,"How is unsupervised learning being used in anomaly detection? - Clustering algorithms
- Outlier detection 
- One-class classification
- Data mining techniques",Anomaly Detection using Principles of Human Perception,"  In the fields of statistics and unsupervised machine learning a fundamental
and well-studied problem is anomaly detection. Anomalies are difficult to
define, yet many algorithms have been proposed. Underlying the approaches is
the nebulous understanding that anomalies are rare, unusual or inconsistent
with the majority of data. The present work provides a philosophical treatise
to clearly define anomalies and develops an algorithm for their efficient
detection with minimal user intervention. Inspired by the Gestalt School of
Psychology and the Helmholtz principle of human perception, anomalies are
assumed to be observations that are unexpected to occur with respect to certain
groupings made by the majority of the data. Under appropriate random variable
modelling anomalies are directly found in a set of data by a uniform and
independent random assumption of the distribution of constituent elements of
the observations, with anomalies corresponding to those observations where the
expectation of the number of occurrences of the elements in a given view is
$<1$. Starting from fundamental principles of human perception an unsupervised
anomaly detection algorithm is developed that is simple, real-time and
parameter-free. Experiments suggest it as a competing choice for univariate
data with promising results on the detection of global anomalies in
multivariate data.
",8.333333333333334,0.6599401,original,6.8,7.333333333333334,4
How is unsupervised learning being used in anomaly detection?,"How is unsupervised learning being used in anomaly detection? - Clustering algorithms
- Outlier detection 
- One-class classification
- Data mining techniques",Elsa: Energy-based learning for semi-supervised anomaly detection,"  Anomaly detection aims at identifying deviant instances from the normal data
distribution. Many advances have been made in the field, including the
innovative use of unsupervised contrastive learning. However, existing methods
generally assume clean training data and are limited when the data contain
unknown anomalies. This paper presents Elsa, a novel semi-supervised anomaly
detection approach that unifies the concept of energy-based models with
unsupervised contrastive learning. Elsa instills robustness against any data
contamination by a carefully designed fine-tuning step based on the new energy
function that forces the normal data to be divided into classes of prototypes.
Experiments on multiple contamination scenarios show the proposed model
achieves SOTA performance. Extensive analyses also verify the contribution of
each component in the proposed model. Beyond the experiments, we also offer a
theoretical interpretation of why contrastive learning alone cannot detect
anomalies under data contamination.
",9.333333333333334,0.6672338,original,6.8,7.333333333333334,4
How is unsupervised learning being used in anomaly detection?,"How is unsupervised learning being used in anomaly detection? - Clustering algorithms
- Outlier detection 
- One-class classification
- Data mining techniques","Transfer Learning from an Auxiliary Discriminative Task for Unsupervised
  Anomaly Detection","  Unsupervised anomaly detection from high dimensional data like mobility
networks is a challenging task. Study of different approaches of feature
engineering from such high dimensional data have been a focus of research in
this field. This study aims to investigate the transferability of features
learned by network classification to unsupervised anomaly detection. We propose
use of an auxiliary classification task to extract features from unlabelled
data by supervised learning, which can be used for unsupervised anomaly
detection. We validate this approach by designing experiments to detect
anomalies in mobility network data from New York and Taipei, and compare the
results to traditional unsupervised feature learning approaches of PCA and
autoencoders. We find that our feature learning approach yields best anomaly
detection performance for both datasets, outperforming other studied
approaches. This establishes the utility of this approach to feature
engineering, which can be applied to other problems of similar nature.
",9.0,0.651183,original,6.8,7.333333333333334,4
How is unsupervised learning being used in anomaly detection?,"How is unsupervised learning being used in anomaly detection? - Clustering algorithms
- Outlier detection 
- One-class classification
- Data mining techniques",Anomaly Detection by Recombining Gated Unsupervised Experts,"  Anomaly detection has been considered under several extents of prior
knowledge. Unsupervised methods do not require any labelled data, whereas
semi-supervised methods leverage some known anomalies. Inspired by
mixture-of-experts models and the analysis of the hidden activations of neural
networks, we introduce a novel data-driven anomaly detection method called
ARGUE. Our method is not only applicable to unsupervised and semi-supervised
environments, but also profits from prior knowledge of self-supervised
settings. We designed ARGUE as a combination of dedicated expert networks,
which specialise on parts of the input data. For its final decision, ARGUE
fuses the distributed knowledge across the expert systems using a gated
mixture-of-experts architecture. Our evaluation motivates that prior knowledge
about the normal data distribution may be as valuable as known anomalies.
",8.666666666666666,0.638342,original,6.8,7.333333333333334,4
How is unsupervised learning being used in anomaly detection?,"How is unsupervised learning being used in anomaly detection? - Clustering algorithms
- Outlier detection 
- One-class classification
- Data mining techniques",A Bayesian Ensemble for Unsupervised Anomaly Detection,"  Methods for unsupervised anomaly detection suffer from the fact that the data
is unlabeled, making it difficult to assess the optimality of detection
algorithms. Ensemble learning has shown exceptional results in classification
and clustering problems, but has not seen as much research in the context of
outlier detection. Existing methods focus on combining output scores of
individual detectors, but this leads to outputs that are not easily
interpretable. In this paper, we introduce a theoretical foundation for
combining individual detectors with Bayesian classifier combination. Not only
are posterior distributions easily interpreted as the probability distribution
of anomalies, but bias, variance, and individual error rates of detectors are
all easily obtained. Performance on real-world datasets shows high accuracy
across varied types of time series data.
",9.666666666666666,0.62767255,original,6.8,7.333333333333334,4
How is unsupervised learning being used in anomaly detection?,"How is unsupervised learning being used in anomaly detection? - Clustering algorithms
- Outlier detection 
- One-class classification
- Data mining techniques","Integrated Clustering and Anomaly Detection (INCAD) for Streaming Data
  (Revised)","  Most current clustering based anomaly detection methods use scoring schema
and thresholds to classify anomalies. These methods are often tailored to
target specific data sets with ""known"" number of clusters. The paper provides a
streaming clustering and anomaly detection algorithm that does not require
strict arbitrary thresholds on the anomaly scores or knowledge of the number of
clusters while performing probabilistic anomaly detection and clustering
simultaneously. This ensures that the cluster formation is not impacted by the
presence of anomalous data, thereby leading to more reliable definition of
""normal vs abnormal"" behavior. The motivations behind developing the INCAD
model and the path that leads to the streaming model is discussed.
",8.333333333333334,0.6195929,expanded,6.8,7.333333333333334,4
How is unsupervised learning being used in anomaly detection?,"How is unsupervised learning being used in anomaly detection? - Clustering algorithms
- Outlier detection 
- One-class classification
- Data mining techniques",Robust Outlier Detection Technique in Data Mining: A Univariate Approach,"  Outliers are the points which are different from or inconsistent with the
rest of the data. They can be novel, new, abnormal, unusual or noisy
information. Outliers are sometimes more interesting than the majority of the
data. The main challenges of outlier detection with the increasing complexity,
size and variety of datasets, are how to catch similar outliers as a group, and
how to evaluate the outliers. This paper describes an approach which uses
Univariate outlier detection as a pre-processing step to detect the outlier and
then applies K-means algorithm hence to analyse the effects of the outliers on
the cluster analysis of dataset.
",5.666666666666667,0.44269013,expanded,6.8,7.333333333333334,4
How is unsupervised learning being used in anomaly detection?,"How is unsupervised learning being used in anomaly detection? - Clustering algorithms
- Outlier detection 
- One-class classification
- Data mining techniques",A Computational Theory and Semi-Supervised Algorithm for Clustering,"  A computational theory for clustering and a semi-supervised clustering
algorithm is presented. Clustering is defined to be the obtainment of groupings
of data such that each group contains no anomalies with respect to a chosen
grouping principle and measure; all other examples are considered to be fringe
points, isolated anomalies, anomalous clusters or unknown clusters. More
precisely, after appropriate modelling under the assumption of uniform random
distribution, any example whose expectation of occurrence is <1 with respect to
a group is considered an anomaly; otherwise it is assigned a membership of that
group. Thus, clustering is conceived as the dual of anomaly detection. The
representation of data is taken to be the Euclidean distance of a point to a
cluster median. This is due to the robustness properties of the median to
outliers, its approximate location of centrality and so that decision
boundaries are general purpose. The kernel of the clustering method is
Mohammad's anomaly detection algorithm, resulting in a parameter-free, fast,
and efficient clustering algorithm. Acknowledging that clustering is an
interactive and iterative process, the algorithm relies on a small fraction of
known relationships between examples. These relationships serve as seeds to
define the user's objectives and guide the clustering process. The algorithm
then expands the clusters accordingly, leaving the remaining examples for
exploration and subsequent iterations. Results are presented on synthetic and
realworld data sets, demonstrating the advantages over the most widely used
clustering methods.
",6.333333333333333,0.5659661,expanded,6.8,7.333333333333334,4
How is unsupervised learning being used in anomaly detection?,"How is unsupervised learning being used in anomaly detection? - Clustering algorithms
- Outlier detection 
- One-class classification
- Data mining techniques","Unsupervised Anomaly Detectors to Detect Intrusions in the Current
  Threat Landscape","  Anomaly detection aims at identifying unexpected fluctuations in the expected
behavior of a given system. It is acknowledged as a reliable answer to the
identification of zero-day attacks to such extent, several ML algorithms that
suit for binary classification have been proposed throughout years. However,
the experimental comparison of a wide pool of unsupervised algorithms for
anomaly-based intrusion detection against a comprehensive set of attacks
datasets was not investigated yet. To fill such gap, we exercise seventeen
unsupervised anomaly detection algorithms on eleven attack datasets. Results
allow elaborating on a wide range of arguments, from the behavior of the
individual algorithm to the suitability of the datasets to anomaly detection.
We conclude that algorithms as Isolation Forests, One-Class Support Vector
Machines and Self-Organizing Maps are more effective than their counterparts
for intrusion detection, while clustering algorithms represent a good
alternative due to their low computational complexity. Further, we detail how
attacks with unstable, distributed or non-repeatable behavior as Fuzzing, Worms
and Botnets are more difficult to detect. Ultimately, we digress on
capabilities of algorithms in detecting anomalies generated by a wide pool of
unknown attacks, showing that achieved metric scores do not vary with respect
to identifying single attacks.
",9.0,0.61289847,expanded,6.8,7.333333333333334,4
How is unsupervised learning being used in anomaly detection?,"How is unsupervised learning being used in anomaly detection? - Clustering algorithms
- Outlier detection 
- One-class classification
- Data mining techniques","Unsupervised anomaly detection algorithms on real-world data: how many
  do we need?","  In this study we evaluate 32 unsupervised anomaly detection algorithms on 52
real-world multivariate tabular datasets, performing the largest comparison of
unsupervised anomaly detection algorithms to date. On this collection of
datasets, the $k$-thNN (distance to the $k$-nearest neighbor) algorithm
significantly outperforms the most other algorithms. Visualizing and then
clustering the relative performance of the considered algorithms on all
datasets, we identify two clear clusters: one with ``local'' datasets, and
another with ``global'' datasets. ``Local'' anomalies occupy a region with low
density when compared to nearby samples, while ``global'' occupy an overall low
density region in the feature space. On the local datasets the $k$NN
($k$-nearest neighbor) algorithm comes out on top. On the global datasets, the
EIF (extended isolation forest) algorithm performs the best. Also taking into
consideration the algorithms' computational complexity, a toolbox with these
three unsupervised anomaly detection algorithms suffices for finding anomalies
in this representative collection of multivariate datasets. By providing access
to code and datasets, our study can be easily reproduced and extended with more
algorithms and/or datasets.
",8.666666666666666,0.63992393,expanded,6.8,7.333333333333334,4
What are the latest approaches to causal inference in data science?,"What are the latest approaches to causal inference in data science? - Counterfactual reasoning
- Treatment effect estimation
- Causal modeling
- Propensity score matching
- Instrumental variables",A Primer on Causality in Data Science,"  Many questions in Data Science are fundamentally causal in that our objective
is to learn the effect of some exposure, randomized or not, on an outcome
interest. Even studies that are seemingly non-causal, such as those with the
goal of prediction or prevalence estimation, have causal elements, including
differential censoring or measurement. As a result, we, as Data Scientists,
need to consider the underlying causal mechanisms that gave rise to the data,
rather than simply the pattern or association observed in those data. In this
work, we review the 'Causal Roadmap' of Petersen and van der Laan (2014) to
provide an introduction to some key concepts in causal inference. Similar to
other causal frameworks, the steps of the Roadmap include clearly stating the
scientific question, defining of the causal model, translating the scientific
question into a causal parameter, assessing the assumptions needed to express
the causal parameter as a statistical estimand, implementation of statistical
estimators including parametric and semi-parametric methods, and interpretation
of our findings. We believe that using such a framework in Data Science will
help to ensure that our statistical analyses are guided by the scientific
question driving our research, while avoiding over-interpreting our results. We
focus on the effect of an exposure occurring at a single time point and
highlight the use of targeted maximum likelihood estimation (TMLE) with Super
Learner.
",7.0,0.67350847,original,6.8,7.333333333333334,4
What are the latest approaches to causal inference in data science?,"What are the latest approaches to causal inference in data science? - Counterfactual reasoning
- Treatment effect estimation
- Causal modeling
- Propensity score matching
- Instrumental variables",A Primer on Causal Analysis,"  We provide a conceptual map to navigate causal analysis problems. Focusing on
the case of discrete random variables, we consider the case of causal effect
estimation from observational data. The presented approaches apply also to
continuous variables, but the issue of estimation becomes more complex. We then
introduce the four schools of thought for causal analysis
",6.333333333333333,0.696984,original,6.8,7.333333333333334,4
What are the latest approaches to causal inference in data science?,"What are the latest approaches to causal inference in data science? - Counterfactual reasoning
- Treatment effect estimation
- Causal modeling
- Propensity score matching
- Instrumental variables",Causality and Statistical Learning,"  We review some approaches and philosophies of causal inference coming from
sociology, economics, computer science, cognitive science, and statistics
",6.666666666666667,0.7257515,original,6.8,7.333333333333334,4
What are the latest approaches to causal inference in data science?,"What are the latest approaches to causal inference in data science? - Counterfactual reasoning
- Treatment effect estimation
- Causal modeling
- Propensity score matching
- Instrumental variables",A Survey on Causal Discovery: Theory and Practice,"  Understanding the laws that govern a phenomenon is the core of scientific
progress. This is especially true when the goal is to model the interplay
between different aspects in a causal fashion. Indeed, causal inference itself
is specifically designed to quantify the underlying relationships that connect
a cause to its effect. Causal discovery is a branch of the broader field of
causality in which causal graphs is recovered from data (whenever possible),
enabling the identification and estimation of causal effects. In this paper, we
explore recent advancements in a unified manner, provide a consistent overview
of existing algorithms developed under different settings, report useful tools
and data, present real-world applications to understand why and how these
methods can be fruitfully exploited.
",8.333333333333334,0.6950414,original,6.8,7.333333333333334,4
What are the latest approaches to causal inference in data science?,"What are the latest approaches to causal inference in data science? - Counterfactual reasoning
- Treatment effect estimation
- Causal modeling
- Propensity score matching
- Instrumental variables","Data-Driven Causal Effect Estimation Based on Graphical Causal
  Modelling: A Survey","  In many fields of scientific research and real-world applications, unbiased
estimation of causal effects from non-experimental data is crucial for
understanding the mechanism underlying the data and for decision-making on
effective responses or interventions. A great deal of research has been
conducted to address this challenging problem from different angles. For
estimating causal effect in observational data, assumptions such as Markov
condition, faithfulness and causal sufficiency are always made. Under the
assumptions, full knowledge such as, a set of covariates or an underlying
causal graph, is typically required. A practical challenge is that in many
applications, no such full knowledge or only some partial knowledge is
available. In recent years, research has emerged to use search strategies based
on graphical causal modelling to discover useful knowledge from data for causal
effect estimation, with some mild assumptions, and has shown promise in
tackling the practical challenge. In this survey, we review these data-driven
methods on causal effect estimation for a single treatment with a single
outcome of interest and focus on the challenges faced by data-driven causal
effect estimation. We concisely summarise the basic concepts and theories that
are essential for data-driven causal effect estimation using graphical causal
modelling but are scattered around the literature. We identify and discuss the
challenges faced by data-driven causal effect estimation and characterise the
existing methods by their assumptions and the approaches to tackling the
challenges. We analyse the strengths and limitations of the different types of
methods and present an empirical evaluation to support the discussions. We hope
this review will motivate more researchers to design better data-driven methods
based on graphical causal modelling for the challenging problem of causal
effect estimation.
",9.0,0.63650924,overlap,6.8,7.333333333333334,4
What are the latest approaches to causal inference in data science?,"What are the latest approaches to causal inference in data science? - Counterfactual reasoning
- Treatment effect estimation
- Causal modeling
- Propensity score matching
- Instrumental variables","How and Why to Use Experimental Data to Evaluate Methods for
  Observational Causal Inference","  Methods that infer causal dependence from observational data are central to
many areas of science, including medicine, economics, and the social sciences.
A variety of theoretical properties of these methods have been proven, but
empirical evaluation remains a challenge, largely due to the lack of
observational data sets for which treatment effect is known. We describe and
analyze observational sampling from randomized controlled trials (OSRCT), a
method for evaluating causal inference methods using data from randomized
controlled trials (RCTs). This method can be used to create constructed
observational data sets with corresponding unbiased estimates of treatment
effect, substantially increasing the number of data sets available for
empirical evaluation of causal inference methods. We show that, in expectation,
OSRCT creates data sets that are equivalent to those produced by randomly
sampling from empirical data sets in which all potential outcomes are
available. We then perform a large-scale evaluation of seven causal inference
methods over 37 data sets, drawn from RCTs, as well as simulators, real-world
computational systems, and observational data sets augmented with a synthetic
response variable. We find notable performance differences when comparing
across data from different sources, demonstrating the importance of using data
from a variety of sources when evaluating any causal inference method.
",7.333333333333333,0.64475465,expanded,6.8,7.333333333333334,4
What are the latest approaches to causal inference in data science?,"What are the latest approaches to causal inference in data science? - Counterfactual reasoning
- Treatment effect estimation
- Causal modeling
- Propensity score matching
- Instrumental variables","On Multi-Cause Causal Inference with Unobserved Confounding:
  Counterexamples, Impossibility, and Alternatives","  Unobserved confounding is a central barrier to drawing causal inferences from
observational data. Several authors have recently proposed that this barrier
can be overcome in the case where one attempts to infer the effects of several
variables simultaneously. In this paper, we present two simple, analytical
counterexamples that challenge the general claims that are central to these
approaches. In addition, we show that nonparametric identification is
impossible in this setting. We discuss practical implications, and suggest
alternatives to the methods that have been proposed so far in this line of
work: using proxy variables and shifting focus to sensitivity analysis.
",8.333333333333334,0.6018542,expanded,6.8,7.333333333333334,4
What are the latest approaches to causal inference in data science?,"What are the latest approaches to causal inference in data science? - Counterfactual reasoning
- Treatment effect estimation
- Causal modeling
- Propensity score matching
- Instrumental variables",CausalNLP: A Practical Toolkit for Causal Inference with Text,"  Causal inference is the process of estimating the effect or impact of a
treatment on an outcome with other covariates as potential confounders (and
mediators) that may need to be controlled. The vast majority of existing
methods and systems for causal inference assume that all variables under
consideration are categorical or numerical (e.g., gender, price, enrollment).
In this paper, we present CausalNLP, a toolkit for inferring causality with
observational data that includes text in addition to traditional numerical and
categorical variables. CausalNLP employs the use of meta learners for treatment
effect estimation and supports using raw text and its linguistic properties as
a treatment, an outcome, or a ""controlled-for"" variable (e.g., confounder). The
library is open source and available at: https://github.com/amaiya/causalnlp.
",8.0,0.62729496,expanded,6.8,7.333333333333334,4
What are the latest approaches to causal inference in data science?,"What are the latest approaches to causal inference in data science? - Counterfactual reasoning
- Treatment effect estimation
- Causal modeling
- Propensity score matching
- Instrumental variables",Causal Inference through a Witness Protection Program,"  One of the most fundamental problems in causal inference is the estimation of
a causal effect when variables are confounded. This is difficult in an
observational study, because one has no direct evidence that all confounders
have been adjusted for. We introduce a novel approach for estimating causal
effects that exploits observational conditional independencies to suggest
""weak"" paths in a unknown causal graph. The widely used faithfulness condition
of Spirtes et al. is relaxed to allow for varying degrees of ""path
cancellations"" that imply conditional independencies but do not rule out the
existence of confounding causal paths. The outcome is a posterior distribution
over bounds on the average causal effect via a linear programming approach and
Bayesian inference. We claim this approach should be used in regular practice
along with other default tools in observational studies.
",8.666666666666666,0.6529753,expanded,6.8,7.333333333333334,4
How is AI being used to optimize supply chain management?,"How is AI being used to optimize supply chain management? - machine learning
- predictive analytics
- inventory management
- demand forecasting
- production planning
- logistics optimization
- artificial intelligence applications","Evaluation of key impression of resilient supply chain based on
  artificial intelligence of things (AIoT)","  In recent years, the high complexity of the business environment, dynamism
and environmental change, uncertainty and concepts such as globalization and
increasing competition of organizations in the national and international arena
have caused many changes in the equations governing the supply chain. In this
case, supply chain organizations must always be prepared for a variety of
challenges and dynamic environmental changes. One of the effective solutions to
face these challenges is to create a resilient supply chain. Resilient supply
chain is able to overcome uncertainties and disruptions in the business
environment. The competitive advantage of this supply chain does not depend
only on low costs, high quality, reduced latency and high level of service.
Rather, it has the ability of the chain to avoid catastrophes and overcome
critical situations, and this is the resilience of the supply chain. AI and IoT
technologies and their combination, called AIoT, have played a key role in
improving supply chain performance in recent years and can therefore increase
supply chain resilience. For this reason, in this study, an attempt was made to
better understand the impact of these technologies on equity by examining the
dimensions and components of the Artificial Intelligence of Things (AIoT)-based
supply chain. Finally, using nonlinear fuzzy decision making method, the most
important components of the impact on the resilient smart supply chain are
determined. Understanding this assessment can help empower the smart supply
chain.
",7.666666666666667,0.6396922,original,6.8,7.333333333333334,4
How is AI being used to optimize supply chain management?,"How is AI being used to optimize supply chain management? - machine learning
- predictive analytics
- inventory management
- demand forecasting
- production planning
- logistics optimization
- artificial intelligence applications","AI in Supply Chain Risk Assessment: A Systematic Literature Review and
  Bibliometric Analysis","  Supply chain risk assessment (SCRA) has witnessed a profound evolution
through the integration of artificial intelligence (AI) and machine learning
(ML) techniques, revolutionizing predictive capabilities and risk mitigation
strategies. The significance of this evolution stems from the critical role of
robust risk management strategies in ensuring operational resilience and
continuity within modern supply chains. Previous reviews have outlined
established methodologies but have overlooked emerging AI/ML techniques,
leaving a notable research gap in understanding their practical implications
within SCRA. This paper conducts a systematic literature review combined with a
comprehensive bibliometric analysis. We meticulously examined 1,717 papers and
derived key insights from a select group of 48 articles published between 2014
and 2023. The review fills this research gap by addressing pivotal research
questions, and exploring existing AI/ML techniques, methodologies, findings,
and future trajectories, thereby providing a more encompassing view of the
evolving landscape of SCRA. Our study unveils the transformative impact of
AI/ML models, such as Random Forest, XGBoost, and hybrids, in substantially
enhancing precision within SCRA. It underscores adaptable post-COVID
strategies, advocating for resilient contingency plans and aligning with
evolving risk landscapes. Significantly, this review surpasses previous
examinations by accentuating emerging AI/ML techniques and their practical
implications within SCRA. Furthermore, it highlights the contributions through
a comprehensive bibliometric analysis, revealing publication trends,
influential authors, and highly cited articles.
",8.0,0.6175312,original,6.8,7.333333333333334,4
How is AI being used to optimize supply chain management?,"How is AI being used to optimize supply chain management? - machine learning
- predictive analytics
- inventory management
- demand forecasting
- production planning
- logistics optimization
- artificial intelligence applications","Artificial Intelligence in Reverse Supply Chain Management: The State of
  the Art","  Product take-back legislation forces manufacturers to bear the costs of
collection and disposal of products that have reached the end of their useful
lives. In order to reduce these costs, manufacturers can consider reuse,
remanufacturing and/or recycling of components as an alternative to disposal.
The implementation of such alternatives usually requires an appropriate reverse
supply chain management. With the concepts of reverse supply chain are gaining
popularity in practice, the use of artificial intelligence approaches in these
areas is also becoming popular. As a result, the purpose of this paper is to
give an overview of the recent publications concerning the application of
artificial intelligence techniques to reverse supply chain with emphasis on
certain types of product returns.
",7.333333333333333,0.5796164,overlap,6.8,7.333333333333334,4
How is AI being used to optimize supply chain management?,"How is AI being used to optimize supply chain management? - machine learning
- predictive analytics
- inventory management
- demand forecasting
- production planning
- logistics optimization
- artificial intelligence applications","Implementing Reinforcement Learning Algorithms in Retail Supply Chains
  with OpenAI Gym Toolkit","  From cutting costs to improving customer experience, forecasting is the crux
of retail supply chain management (SCM) and the key to better supply chain
performance. Several retailers are using AI/ML models to gather datasets and
provide forecast guidance in applications such as Cognitive Demand Forecasting,
Product End-of-Life, Forecasting, and Demand Integrated Product Flow. Early
work in these areas looked at classical algorithms to improve on a gamut of
challenges such as network flow and graphs. But the recent disruptions have
made it critical for supply chains to have the resiliency to handle unexpected
events. The biggest challenge lies in matching supply with demand.
  Reinforcement Learning (RL) with its ability to train systems to respond to
unforeseen environments, is being increasingly adopted in SCM to improve
forecast accuracy, solve supply chain optimization challenges, and train
systems to respond to unforeseen circumstances. Companies like UPS and Amazon
have developed RL algorithms to define winning AI strategies and keep up with
rising consumer delivery expectations. While there are many ways to build RL
algorithms for supply chain use cases, the OpenAI Gym toolkit is becoming the
preferred choice because of the robust framework for event-driven simulations.
  This white paper explores the application of RL in supply chain forecasting
and describes how to build suitable RL models and algorithms by using the
OpenAI Gym toolkit.
",9.0,0.6298524,overlap,6.8,7.333333333333334,4
How is AI being used to optimize supply chain management?,"How is AI being used to optimize supply chain management? - machine learning
- predictive analytics
- inventory management
- demand forecasting
- production planning
- logistics optimization
- artificial intelligence applications","Enterprise AI Canvas -- Integrating Artificial Intelligence into
  Business","  Artificial Intelligence (AI) and Machine Learning have enormous potential to
transform businesses and disrupt entire industry sectors. However, companies
wishing to integrate algorithmic decisions into their face multiple challenges:
They have to identify use-cases in which artificial intelligence can create
value, as well as decisions that can be supported or executed automatically.
Furthermore, the organization will need to be transformed to be able to
integrate AI based systems into their human work-force. Furthermore, the more
technical aspects of the underlying machine learning model have to be discussed
in terms of how they impact the various units of a business: Where do the
relevant data come from, which constraints have to be considered, how is the
quality of the data and the prediction evaluated?
  The Enterprise AI canvas is designed to bring Data Scientist and business
expert together to discuss and define all relevant aspects which need to be
clarified in order to integrate AI based systems into a digital enterprise. It
consists of two parts where part one focuses on the business view and
organizational aspects, whereas part two focuses on the underlying machine
learning model and the data it uses.
",6.333333333333333,0.5883844,original,6.8,7.333333333333334,4
How is AI being used to optimize supply chain management?,"How is AI being used to optimize supply chain management? - machine learning
- predictive analytics
- inventory management
- demand forecasting
- production planning
- logistics optimization
- artificial intelligence applications","Optimizing Inventory Routing: A Decision-Focused Learning Approach using
  Neural Networks","  Inventory Routing Problem (IRP) is a crucial challenge in supply chain
management as it involves optimizing efficient route selection while
considering the uncertainty of inventory demand planning. To solve IRPs,
usually a two-stage approach is employed, where demand is predicted using
machine learning techniques first, and then an optimization algorithm is used
to minimize routing costs. Our experiment shows machine learning models fall
short of achieving perfect accuracy because inventory levels are influenced by
the dynamic business environment, which, in turn, affects the optimization
problem in the next stage, resulting in sub-optimal decisions. In this paper,
we formulate and propose a decision-focused learning-based approach to solving
real-world IRPs. This approach directly integrates inventory prediction and
routing optimization within an end-to-end system potentially ensuring a robust
supply chain strategy.
",9.0,0.5790982,expanded,6.8,7.333333333333334,4
How is AI being used to optimize supply chain management?,"How is AI being used to optimize supply chain management? - machine learning
- predictive analytics
- inventory management
- demand forecasting
- production planning
- logistics optimization
- artificial intelligence applications","The challenges and realities of retailing in a COVID-19 world:
  Identifying trending and Vital During Crisis keywords during Covid-19 using
  Machine Learning (Austria as a case study)","  From global pandemics to geopolitical turmoil, leaders in logistics, product
allocation, procurement and operations are facing increasing difficulty with
safeguarding their organizations against supply chain vulnerabilities. It is
recommended to opt for forecasting against trending based benchmark because
auditing a future forecast puts more focus on seasonality. The forecasting
models provide with end-to-end, real time oversight of the entire supply chain,
while utilizing predictive analytics and artificial intelligence to identify
potential disruptions before they occur. By combining internal and external
data points, coming up with an AI-enabled modelling engine can greatly reduce
risk by helping retail companies proactively respond to supply and demand
variability. This research paper puts focus on creating an ingenious way to
tackle the impact of COVID19 on Supply chain, product allocation, trending and
seasonality.
  Key words: Supply chain, covid-19, forecasting, coronavirus, manufacturing,
seasonality, trending, retail.
",8.0,0.60627794,expanded,6.8,7.333333333333334,4
How is AI being used to optimize supply chain management?,"How is AI being used to optimize supply chain management? - machine learning
- predictive analytics
- inventory management
- demand forecasting
- production planning
- logistics optimization
- artificial intelligence applications","Comparing Deep Reinforcement Learning Algorithms in Two-Echelon Supply
  Chains","  In this study, we analyze and compare the performance of state-of-the-art
deep reinforcement learning algorithms for solving the supply chain inventory
management problem. This complex sequential decision-making problem consists of
determining the optimal quantity of products to be produced and shipped across
different warehouses over a given time horizon. In particular, we present a
mathematical formulation of a two-echelon supply chain environment with
stochastic and seasonal demand, which allows managing an arbitrary number of
warehouses and product types. Through a rich set of numerical experiments, we
compare the performance of different deep reinforcement learning algorithms
under various supply chain structures, topologies, demands, capacities, and
costs. The results of the experimental plan indicate that deep reinforcement
learning algorithms outperform traditional inventory management strategies,
such as the static (s, Q)-policy. Furthermore, this study provides detailed
insight into the design and development of an open-source software library that
provides a customizable environment for solving the supply chain inventory
management problem using a wide range of data-driven approaches.
",9.0,0.59736645,expanded,6.8,7.333333333333334,4
What are novel techniques for feature engineering in time series data?,"What are novel techniques for feature engineering in time series data? - Time series analysis
- Feature extraction
- Machine learning
- Signal processing
- Data preprocessing
- Feature selection
- Temporal data mining
- Time series forecasting",Feature Programming for Multivariate Time Series Prediction,"  We introduce the concept of programmable feature engineering for time series
modeling and propose a feature programming framework. This framework generates
large amounts of predictive features for noisy multivariate time series while
allowing users to incorporate their inductive bias with minimal effort. The key
motivation of our framework is to view any multivariate time series as a
cumulative sum of fine-grained trajectory increments, with each increment
governed by a novel spin-gas dynamical Ising model. This fine-grained
perspective motivates the development of a parsimonious set of operators that
summarize multivariate time series in an abstract fashion, serving as the
foundation for large-scale automated feature engineering. Numerically, we
validate the efficacy of our method on several synthetic and real-world noisy
time series datasets.
",8.333333333333334,0.71222615,original,6.8,7.333333333333334,4
What are novel techniques for feature engineering in time series data?,"What are novel techniques for feature engineering in time series data? - Time series analysis
- Feature extraction
- Machine learning
- Signal processing
- Data preprocessing
- Feature selection
- Temporal data mining
- Time series forecasting",AutoFITS: Automatic Feature Engineering for Irregular Time Series,"  A time series represents a set of observations collected over time.
Typically, these observations are captured with a uniform sampling frequency
(e.g. daily). When data points are observed in uneven time intervals the time
series is referred to as irregular or intermittent. In such scenarios, the most
common solution is to reconstruct the time series to make it regular, thus
removing its intermittency. We hypothesise that, in irregular time series, the
time at which each observation is collected may be helpful to summarise the
dynamics of the data and improve forecasting performance. We study this idea by
developing a novel automatic feature engineering framework, which focuses on
extracting information from this point of view, i.e., when each instance is
collected. We study how valuable this information is by integrating it in a
time series forecasting workflow and investigate how it compares to or
complements state-of-the-art methods for regular time series forecasting. In
the end, we contribute by providing a novel framework that tackles feature
engineering for time series from an angle previously vastly ignored. We show
that our approach has the potential to further extract more information about
time series that significantly improves forecasting performance.
",8.333333333333334,0.669768,original,6.8,7.333333333333334,4
What are novel techniques for feature engineering in time series data?,"What are novel techniques for feature engineering in time series data? - Time series analysis
- Feature extraction
- Machine learning
- Signal processing
- Data preprocessing
- Feature selection
- Temporal data mining
- Time series forecasting","Feature Engineering Methods on Multivariate Time-Series Data for
  Financial Data Science Competitions","  This paper is a work in progress. We are looking for collaborators to provide
us financial datasets in Equity/Futures market to conduct more bench-marking
studies. The authors have papers employing similar methods applied on the
Numerai dataset, which is freely available but obfuscated.
  We apply different feature engineering methods for time-series to US market
price data. The predictive power of models are tested against Numerai-Signals
targets.
",5.666666666666667,0.53102374,overlap,6.8,7.333333333333334,4
What are novel techniques for feature engineering in time series data?,"What are novel techniques for feature engineering in time series data? - Time series analysis
- Feature extraction
- Machine learning
- Signal processing
- Data preprocessing
- Feature selection
- Temporal data mining
- Time series forecasting",Forecasting large collections of time series: feature-based methods,"  In economics and many other forecasting domains, the real world problems are
too complex for a single model that assumes a specific data generation process.
The forecasting performance of different methods changes depending on the
nature of the time series. When forecasting large collections of time series,
two lines of approaches have been developed using time series features, namely
feature-based model selection and feature-based model combination. This chapter
discusses the state-of-the-art feature-based methods, with reference to
open-source software implementations.
",7.0,0.6472578,overlap,6.8,7.333333333333334,4
What are novel techniques for feature engineering in time series data?,"What are novel techniques for feature engineering in time series data? - Time series analysis
- Feature extraction
- Machine learning
- Signal processing
- Data preprocessing
- Feature selection
- Temporal data mining
- Time series forecasting",Feature-based time-series analysis,"  This work presents an introduction to feature-based time-series analysis. The
time series as a data type is first described, along with an overview of the
interdisciplinary time-series analysis literature. I then summarize the range
of feature-based representations for time series that have been developed to
aid interpretable insights into time-series structure. Particular emphasis is
given to emerging research that facilitates wide comparison of feature-based
representations that allow us to understand the properties of a time-series
dataset that make it suited to a particular feature-based representation or
analysis algorithm. The future of time-series analysis is likely to embrace
approaches that exploit machine learning methods to partially automate human
learning to aid understanding of the complex dynamical patterns in the time
series we measure from the world.
",5.333333333333333,0.6587908,overlap,6.8,7.333333333333334,4
What are novel techniques for feature engineering in time series data?,"What are novel techniques for feature engineering in time series data? - Time series analysis
- Feature extraction
- Machine learning
- Signal processing
- Data preprocessing
- Feature selection
- Temporal data mining
- Time series forecasting","Automatic Feature Engineering for Time Series Classification: Evaluation
  and Discussion","  Time Series Classification (TSC) has received much attention in the past two
decades and is still a crucial and challenging problem in data science and
knowledge engineering. Indeed, along with the increasing availability of time
series data, many TSC algorithms have been suggested by the research community
in the literature. Besides state-of-the-art methods based on similarity
measures, intervals, shapelets, dictionaries, deep learning methods or hybrid
ensemble methods, several tools for extracting unsupervised informative summary
statistics, aka features, from time series have been designed in the recent
years. Originally designed for descriptive analysis and visualization of time
series with informative and interpretable features, very few of these feature
engineering tools have been benchmarked for TSC problems and compared with
state-of-the-art TSC algorithms in terms of predictive performance. In this
article, we aim at filling this gap and propose a simple TSC process to
evaluate the potential predictive performance of the feature sets obtained with
existing feature engineering tools. Thus, we present an empirical study of 11
feature engineering tools branched with 9 supervised classifiers over 112 time
series data sets. The analysis of the results of more than 10000 learning
experiments indicate that feature-based methods perform as accurately as
current state-of-the-art TSC algorithms, and thus should rightfully be
considered further in the TSC literature.
",7.333333333333333,0.6428358,expanded,6.8,7.333333333333334,4
What are novel techniques for feature engineering in time series data?,"What are novel techniques for feature engineering in time series data? - Time series analysis
- Feature extraction
- Machine learning
- Signal processing
- Data preprocessing
- Feature selection
- Temporal data mining
- Time series forecasting","Applying Nature-Inspired Optimization Algorithms for Selecting Important
  Timestamps to Reduce Time Series Dimensionality","  Time series data account for a major part of data supply available today.
Time series mining handles several tasks such as classification, clustering,
query-by-content, prediction, and others. Performing data mining tasks on raw
time series is inefficient as these data are high-dimensional by nature.
Instead, time series are first pre-processed using several techniques before
different data mining tasks can be performed on them. In general, there are two
main approaches to reduce time series dimensionality, the first is what we call
landmark methods. These methods are based on finding characteristic features in
the target time series. The second is based on data transformations. These
methods transform the time series from the original space into a reduced space,
where they can be managed more efficiently. The method we present in this paper
applies a third approach, as it projects a time series onto a lower-dimensional
space by selecting important points in the time series. The novelty of our
method is that these points are not chosen according to a geometric criterion,
which is subjective in most cases, but through an optimization process. The
other important characteristic of our method is that these important points are
selected on a dataset-level and not on a single time series-level. The direct
advantage of this strategy is that the distance defined on the low-dimensional
space lower bounds the original distance applied to raw data. This enables us
to apply the popular GEMINI algorithm. The promising results of our experiments
on a wide variety of time series datasets, using different optimizers, and
applied to the two major data mining tasks, validate our new method.
",8.333333333333334,0.6387098,expanded,6.8,7.333333333333334,4
How can GANs (Generative Adversarial Networks) improve image resolution?,"How can GANs (Generative Adversarial Networks) improve image resolution? - Neural networks
- Super-resolution
- Image enhancement
- Deep learning
- Computer vision
- GAN architecture
- High-resolution images
- Image processing techniques",Generative Adversarial Networks for Image Super-Resolution: A Survey,"  Single image super-resolution (SISR) has played an important role in the
field of image processing. Recent generative adversarial networks (GANs) can
achieve excellent results on low-resolution images with small samples. However,
there are little literatures summarizing different GANs in SISR. In this paper,
we conduct a comparative study of GANs from different perspectives. We first
take a look at developments of GANs. Second, we present popular architectures
for GANs in big and small samples for image applications. Then, we analyze
motivations, implementations and differences of GANs based optimization methods
and discriminative learning for image super-resolution in terms of supervised,
semi-supervised and unsupervised manners. Next, we compare performance of these
popular GANs on public datasets via quantitative and qualitative analysis in
SISR. Finally, we highlight challenges of GANs and potential research points
for SISR.
",9.666666666666666,0.7283154,overlap,6.8,7.333333333333334,4
How can GANs (Generative Adversarial Networks) improve image resolution?,"How can GANs (Generative Adversarial Networks) improve image resolution? - Neural networks
- Super-resolution
- Image enhancement
- Deep learning
- Computer vision
- GAN architecture
- High-resolution images
- Image processing techniques","Resolution Dependent GAN Interpolation for Controllable Image Synthesis
  Between Domains","  GANs can generate photo-realistic images from the domain of their training
data. However, those wanting to use them for creative purposes often want to
generate imagery from a truly novel domain, a task which GANs are inherently
unable to do. It is also desirable to have a level of control so that there is
a degree of artistic direction rather than purely curation of random results.
Here we present a method for interpolating between generative models of the
StyleGAN architecture in a resolution dependent manner. This allows us to
generate images from an entirely novel domain and do this with a degree of
control over the nature of the output.
",6.666666666666667,0.7370848,original,6.8,7.333333333333334,4
How can GANs (Generative Adversarial Networks) improve image resolution?,"How can GANs (Generative Adversarial Networks) improve image resolution? - Neural networks
- Super-resolution
- Image enhancement
- Deep learning
- Computer vision
- GAN architecture
- High-resolution images
- Image processing techniques",Image-Adaptive GAN based Reconstruction,"  In the recent years, there has been a significant improvement in the quality
of samples produced by (deep) generative models such as variational
auto-encoders and generative adversarial networks. However, the representation
capabilities of these methods still do not capture the full distribution for
complex classes of images, such as human faces. This deficiency has been
clearly observed in previous works that use pre-trained generative models to
solve imaging inverse problems. In this paper, we suggest to mitigate the
limited representation capabilities of generators by making them image-adaptive
and enforcing compliance of the restoration with the observations via
back-projections. We empirically demonstrate the advantages of our proposed
approach for image super-resolution and compressed sensing.
",6.333333333333333,0.6982577,original,6.8,7.333333333333334,4
How can GANs (Generative Adversarial Networks) improve image resolution?,"How can GANs (Generative Adversarial Networks) improve image resolution? - Neural networks
- Super-resolution
- Image enhancement
- Deep learning
- Computer vision
- GAN architecture
- High-resolution images
- Image processing techniques","A General Method to Incorporate Spatial Information into Loss Functions
  for GAN-based Super-resolution Models","  Generative Adversarial Networks (GANs) have shown great performance on
super-resolution problems since they can generate more visually realistic
images and video frames. However, these models often introduce side effects
into the outputs, such as unexpected artifacts and noises. To reduce these
artifacts and enhance the perceptual quality of the results, in this paper, we
propose a general method that can be effectively used in most GAN-based
super-resolution (SR) models by introducing essential spatial information into
the training process. We extract spatial information from the input data and
incorporate it into the training loss, making the corresponding loss a
spatially adaptive (SA) one. After that, we utilize it to guide the training
process. We will show that the proposed approach is independent of the methods
used to extract the spatial information and independent of the SR tasks and
models. This method consistently guides the training process towards generating
visually pleasing SR images and video frames, substantially mitigating
artifacts and noise, ultimately leading to enhanced perceptual quality.
",9.0,0.7326975,overlap,6.8,7.333333333333334,4
How can GANs (Generative Adversarial Networks) improve image resolution?,"How can GANs (Generative Adversarial Networks) improve image resolution? - Neural networks
- Super-resolution
- Image enhancement
- Deep learning
- Computer vision
- GAN architecture
- High-resolution images
- Image processing techniques","A Generative Model for Hallucinating Diverse Versions of Super
  Resolution Images","  Traditionally, the main focus of image super-resolution techniques is on
recovering the most likely high-quality images from low-quality images, using a
one-to-one low- to high-resolution mapping. Proceeding that way, we ignore the
fact that there are generally many valid versions of high-resolution images
that map to a given low-resolution image. We are tackling in this work the
problem of obtaining different high-resolution versions from the same
low-resolution image using Generative Adversarial Models. Our learning approach
makes use of high frequencies available in the training high-resolution images
for preserving and exploring in an unsupervised manner the structural
information available within these images. Experimental results on the CelebA
dataset confirm the effectiveness of the proposed method, which allows the
generation of both realistic and diverse high-resolution images from
low-resolution images.
",9.0,0.70288515,original,6.8,7.333333333333334,4
How can GANs (Generative Adversarial Networks) improve image resolution?,"How can GANs (Generative Adversarial Networks) improve image resolution? - Neural networks
- Super-resolution
- Image enhancement
- Deep learning
- Computer vision
- GAN architecture
- High-resolution images
- Image processing techniques",Does Diffusion Beat GAN in Image Super Resolution?,"  There is a prevalent opinion in the recent literature that Diffusion-based
models outperform GAN-based counterparts on the Image Super Resolution (ISR)
problem. However, in most studies, Diffusion-based ISR models were trained
longer and utilized larger networks than the GAN baselines. This raises the
question of whether the superiority of Diffusion models is due to the Diffusion
paradigm being better suited for the ISR task or if it is a consequence of the
increased scale and computational resources used in contemporary studies. In
our work, we compare Diffusion-based and GAN-based Super Resolution under
controlled settings, where both approaches are matched in terms of
architecture, model and dataset size, and computational budget. We show that a
GAN-based model can achieve results comparable to a Diffusion-based model.
Additionally, we explore the impact of design choices such as text conditioning
and augmentation on the performance of ISR models, showcasing their effect on
several downstream tasks. We will release the inference code and weights of our
scaled GAN.
",6.666666666666667,0.67761815,expanded,6.8,7.333333333333334,4
How can GANs (Generative Adversarial Networks) improve image resolution?,"How can GANs (Generative Adversarial Networks) improve image resolution? - Neural networks
- Super-resolution
- Image enhancement
- Deep learning
- Computer vision
- GAN architecture
- High-resolution images
- Image processing techniques",Image Super-Resolution Using VDSR-ResNeXt and SRCGAN,"  Over the past decade, many Super Resolution techniques have been developed
using deep learning. Among those, generative adversarial networks (GAN) and
very deep convolutional networks (VDSR) have shown promising results in terms
of HR image quality and computational speed. In this paper, we propose two
approaches based on these two algorithms: VDSR-ResNeXt, which is a deep
multi-branch convolutional network inspired by VDSR and ResNeXt; and SRCGAN,
which is a conditional GAN that explicitly passes class labels as input to the
GAN. The two methods were implemented on common SR benchmark datasets for both
quantitative and qualitative assessment.
",8.666666666666666,0.68168086,expanded,6.8,7.333333333333334,4
How can GANs (Generative Adversarial Networks) improve image resolution?,"How can GANs (Generative Adversarial Networks) improve image resolution? - Neural networks
- Super-resolution
- Image enhancement
- Deep learning
- Computer vision
- GAN architecture
- High-resolution images
- Image processing techniques",Infrared Image Super-Resolution via GAN,"  The ability of generative models to accurately fit data distributions has
resulted in their widespread adoption and success in fields such as computer
vision and natural language processing. In this chapter, we provide a brief
overview of the application of generative models in the domain of infrared (IR)
image super-resolution, including a discussion of the various challenges and
adversarial training methods employed. We propose potential areas for further
investigation and advancement in the application of generative models for IR
image super-resolution.
",8.333333333333334,0.64804137,expanded,6.8,7.333333333333334,4
What role does data augmentation play in small dataset training?,"What role does data augmentation play in small dataset training? - Machine learning
- Deep learning
- Image processing
- Overfitting
- Transfer learning","How Much Data Are Augmentations Worth? An Investigation into Scaling
  Laws, Invariance, and Implicit Regularization","  Despite the clear performance benefits of data augmentations, little is known
about why they are so effective. In this paper, we disentangle several key
mechanisms through which data augmentations operate. Establishing an exchange
rate between augmented and additional real data, we find that in
out-of-distribution testing scenarios, augmentations which yield samples that
are diverse, but inconsistent with the data distribution can be even more
valuable than additional training data. Moreover, we find that data
augmentations which encourage invariances can be more valuable than invariance
alone, especially on small and medium sized training sets. Following this
observation, we show that augmentations induce additional stochasticity during
training, effectively flattening the loss landscape.
",8.0,0.66949445,original,6.8,7.333333333333334,4
What role does data augmentation play in small dataset training?,"What role does data augmentation play in small dataset training? - Machine learning
- Deep learning
- Image processing
- Overfitting
- Transfer learning",Improved Mixed-Example Data Augmentation,"  In order to reduce overfitting, neural networks are typically trained with
data augmentation, the practice of artificially generating additional training
data via label-preserving transformations of existing training examples. While
these types of transformations make intuitive sense, recent work has
demonstrated that even non-label-preserving data augmentation can be
surprisingly effective, examining this type of data augmentation through linear
combinations of pairs of examples. Despite their effectiveness, little is known
about why such methods work. In this work, we aim to explore a new, more
generalized form of this type of data augmentation in order to determine
whether such linearity is necessary. By considering this broader scope of
""mixed-example data augmentation"", we find a much larger space of practical
augmentation techniques, including methods that improve upon previous
state-of-the-art. This generalization has benefits beyond the promise of
improved performance, revealing a number of types of mixed-example data
augmentation that are radically different from those considered in prior work,
which provides evidence that current theories for the effectiveness of such
methods are incomplete and suggests that any such theory must explain a much
broader phenomenon. Code is available at
https://github.com/ceciliaresearch/MixedExample.
",6.666666666666667,0.67491686,overlap,6.8,7.333333333333334,4
What role does data augmentation play in small dataset training?,"What role does data augmentation play in small dataset training? - Machine learning
- Deep learning
- Image processing
- Overfitting
- Transfer learning",WeMix: How to Better Utilize Data Augmentation,"  Data augmentation is a widely used training trick in deep learning to improve
the network generalization ability. Despite many encouraging results, several
recent studies did point out limitations of the conventional data augmentation
scheme in certain scenarios, calling for a better theoretical understanding of
data augmentation. In this work, we develop a comprehensive analysis that
reveals pros and cons of data augmentation. The main limitation of data
augmentation arises from the data bias, i.e. the augmented data distribution
can be quite different from the original one. This data bias leads to a
suboptimal performance of existing data augmentation methods. To this end, we
develop two novel algorithms, termed ""AugDrop"" and ""MixLoss"", to correct the
data bias in the data augmentation. Our theoretical analysis shows that both
algorithms are guaranteed to improve the effect of data augmentation through
the bias correction, which is further validated by our empirical studies.
Finally, we propose a generic algorithm ""WeMix"" by combining AugDrop and
MixLoss, whose effectiveness is observed from extensive empirical evaluations.
",6.666666666666667,0.6545789,original,6.8,7.333333333333334,4
What role does data augmentation play in small dataset training?,"What role does data augmentation play in small dataset training? - Machine learning
- Deep learning
- Image processing
- Overfitting
- Transfer learning",Data Augmentation as Feature Manipulation,"  Data augmentation is a cornerstone of the machine learning pipeline, yet its
theoretical underpinnings remain unclear. Is it merely a way to artificially
augment the data set size? Or is it about encouraging the model to satisfy
certain invariance? In this work we consider another angle, and we study the
effect of data augmentation on the dynamic of the learning process. We find
that data augmentation can alter the relative importance of various features,
effectively making certain informative but hard to learn features more likely
to be captured in the learning process. Importantly, we show that this effect
is more pronounced for non-linear models, such as neural networks. Our main
contribution is a detailed analysis of data augmentation on the learning
dynamic for a two layer convolutional neural network in the recently proposed
multi-view data model by Allen-Zhu and Li [2020]. We complement this analysis
with further experimental evidence that data augmentation can be viewed as
feature manipulation.
",7.666666666666667,0.6578872,overlap,6.8,7.333333333333334,4
What role does data augmentation play in small dataset training?,"What role does data augmentation play in small dataset training? - Machine learning
- Deep learning
- Image processing
- Overfitting
- Transfer learning",Research Trends and Applications of Data Augmentation Algorithms,"  In the Machine Learning research community, there is a consensus regarding
the relationship between model complexity and the required amount of data and
computation power. In real world applications, these computational requirements
are not always available, motivating research on regularization methods. In
addition, current and past research have shown that simpler classification
algorithms can reach state-of-the-art performance on computer vision tasks
given a robust method to artificially augment the training dataset. Because of
this, data augmentation techniques became a popular research topic in recent
years. However, existing data augmentation methods are generally less
transferable than other regularization methods. In this paper we identify the
main areas of application of data augmentation algorithms, the types of
algorithms used, significant research trends, their progression over time and
research gaps in data augmentation literature. To do this, the related
literature was collected through the Scopus database. Its analysis was done
following network science, text mining and exploratory analysis approaches. We
expect readers to understand the potential of data augmentation, as well as
identify future research directions and open questions within data augmentation
research.
",7.666666666666667,0.6447019,overlap,6.8,7.333333333333334,4
What role does data augmentation play in small dataset training?,"What role does data augmentation play in small dataset training? - Machine learning
- Deep learning
- Image processing
- Overfitting
- Transfer learning",Further advantages of data augmentation on convolutional neural networks,"  Data augmentation is a popular technique largely used to enhance the training
of convolutional neural networks. Although many of its benefits are well known
by deep learning researchers and practitioners, its implicit regularization
effects, as compared to popular explicit regularization techniques, such as
weight decay and dropout, remain largely unstudied. As a matter of fact,
convolutional neural networks for image object classification are typically
trained with both data augmentation and explicit regularization, assuming the
benefits of all techniques are complementary. In this paper, we systematically
analyze these techniques through ablation studies of different network
architectures trained with different amounts of training data. Our results
unveil a largely ignored advantage of data augmentation: networks trained with
just data augmentation more easily adapt to different architectures and amount
of training data, as opposed to weight decay and dropout, which require
specific fine-tuning of their hyperparameters.
",9.0,0.63237804,expanded,6.8,7.333333333333334,4
What role does data augmentation play in small dataset training?,"What role does data augmentation play in small dataset training? - Machine learning
- Deep learning
- Image processing
- Overfitting
- Transfer learning","DualAug: Exploiting Additional Heavy Augmentation with OOD Data
  Rejection","  Data augmentation is a dominant method for reducing model overfitting and
improving generalization. Most existing data augmentation methods tend to find
a compromise in augmenting the data, \textit{i.e.}, increasing the amplitude of
augmentation carefully to avoid degrading some data too much and doing harm to
the model performance. We delve into the relationship between data augmentation
and model performance, revealing that the performance drop with heavy
augmentation comes from the presence of out-of-distribution (OOD) data.
Nonetheless, as the same data transformation has different effects for
different training samples, even for heavy augmentation, there remains part of
in-distribution data which is beneficial to model training. Based on the
observation, we propose a novel data augmentation method, named
\textbf{DualAug}, to keep the augmentation in distribution as much as possible
at a reasonable time and computational cost. We design a data mixing strategy
to fuse augmented data from both the basic- and the heavy-augmentation
branches. Extensive experiments on supervised image classification benchmarks
show that DualAug improve various automated data augmentation method. Moreover,
the experiments on semi-supervised learning and contrastive self-supervised
learning demonstrate that our DualAug can also improve related method. Code is
available at
\href{https://github.com/shuguang99/DualAug}{https://github.com/shuguang99/DualAug}.
",7.0,0.6396676,expanded,6.8,7.333333333333334,4
How is federated learning enhancing privacy in healthcare applications?,"How is federated learning enhancing privacy in healthcare applications? - Machine learning
- Healthcare data
- Data privacy
- Federated learning benefits
- Health informatics
- Patient data privacy",Survey of Privacy Threats and Countermeasures in Federated Learning,"  Federated learning is widely considered to be as a privacy-aware learning
method because no training data is exchanged directly between clients.
Nevertheless, there are threats to privacy in federated learning, and privacy
countermeasures have been studied. However, we note that common and unique
privacy threats among typical types of federated learning have not been
categorized and described in a comprehensive and specific way. In this paper,
we describe privacy threats and countermeasures for the typical types of
federated learning; horizontal federated learning, vertical federated learning,
and transfer federated learning.
",5.666666666666667,0.8171472,original,6.8,7.333333333333334,4
How is federated learning enhancing privacy in healthcare applications?,"How is federated learning enhancing privacy in healthcare applications? - Machine learning
- Healthcare data
- Data privacy
- Federated learning benefits
- Health informatics
- Patient data privacy","Privacy and Fairness in Federated Learning: on the Perspective of
  Trade-off","  Federated learning (FL) has been a hot topic in recent years. Ever since it
was introduced, researchers have endeavored to devise FL systems that protect
privacy or ensure fair results, with most research focusing on one or the
other. As two crucial ethical notions, the interactions between privacy and
fairness are comparatively less studied. However, since privacy and fairness
compete, considering each in isolation will inevitably come at the cost of the
other. To provide a broad view of these two critical topics, we presented a
detailed literature review of privacy and fairness issues, highlighting unique
challenges posed by FL and solutions in federated settings. We further
systematically surveyed different interactions between privacy and fairness,
trying to reveal how privacy and fairness could affect each other and point out
new research directions in fair and private FL.
",6.0,0.7765515,original,6.8,7.333333333333334,4
How is federated learning enhancing privacy in healthcare applications?,"How is federated learning enhancing privacy in healthcare applications? - Machine learning
- Healthcare data
- Data privacy
- Federated learning benefits
- Health informatics
- Patient data privacy","Federated Learning in Healthcare: Model Misconducts, Security,
  Challenges, Applications, and Future Research Directions -- A Systematic
  Review","  Data privacy has become a major concern in healthcare due to the increasing
digitization of medical records and data-driven medical research. Protecting
sensitive patient information from breaches and unauthorized access is
critical, as such incidents can have severe legal and ethical complications.
Federated Learning (FL) addresses this concern by enabling multiple healthcare
institutions to collaboratively learn from decentralized data without sharing
it. FL's scope in healthcare covers areas such as disease prediction, treatment
customization, and clinical trial research. However, implementing FL poses
challenges, including model convergence in non-IID (independent and identically
distributed) data environments, communication overhead, and managing
multi-institutional collaborations. A systematic review of FL in healthcare is
necessary to evaluate how effectively FL can provide privacy while maintaining
the integrity and usability of medical data analysis. In this study, we analyze
existing literature on FL applications in healthcare. We explore the current
state of model security practices, identify prevalent challenges, and discuss
practical applications and their implications. Additionally, the review
highlights promising future research directions to refine FL implementations,
enhance data security protocols, and expand FL's use to broader healthcare
applications, which will benefit future researchers and practitioners.
",9.333333333333334,0.7526212,overlap,6.8,7.333333333333334,4
How is federated learning enhancing privacy in healthcare applications?,"How is federated learning enhancing privacy in healthcare applications? - Machine learning
- Healthcare data
- Data privacy
- Federated learning benefits
- Health informatics
- Patient data privacy",Federated Learning: Opportunities and Challenges,"  Federated Learning (FL) is a concept first introduced by Google in 2016, in
which multiple devices collaboratively learn a machine learning model without
sharing their private data under the supervision of a central server. This
offers ample opportunities in critical domains such as healthcare, finance etc,
where it is risky to share private user information to other organisations or
devices. While FL appears to be a promising Machine Learning (ML) technique to
keep the local data private, it is also vulnerable to attacks like other ML
models. Given the growing interest in the FL domain, this report discusses the
opportunities and challenges in federated learning.
",7.666666666666667,0.74317837,original,6.8,7.333333333333334,4
How is federated learning enhancing privacy in healthcare applications?,"How is federated learning enhancing privacy in healthcare applications? - Machine learning
- Healthcare data
- Data privacy
- Federated learning benefits
- Health informatics
- Patient data privacy","Privacy Risks Analysis and Mitigation in Federated Learning for Medical
  Images","  Federated learning (FL) is gaining increasing popularity in the medical
domain for analyzing medical images, which is considered an effective technique
to safeguard sensitive patient data and comply with privacy regulations.
However, several recent studies have revealed that the default settings of FL
may leak private training data under privacy attacks. Thus, it is still unclear
whether and to what extent such privacy risks of FL exist in the medical
domain, and if so, ""how to mitigate such risks?"". In this paper, first, we
propose a holistic framework for Medical data Privacy risk analysis and
mitigation in Federated Learning (MedPFL) to analyze privacy risks and develop
effective mitigation strategies in FL for protecting private medical data.
Second, we demonstrate the substantial privacy risks of using FL to process
medical images, where adversaries can easily perform privacy attacks to
reconstruct private medical images accurately. Third, we show that the defense
approach of adding random noises may not always work effectively to protect
medical images against privacy attacks in FL, which poses unique and pressing
challenges associated with medical data for privacy protection.
",8.666666666666666,0.7615551,original,6.8,7.333333333333334,4
How is federated learning enhancing privacy in healthcare applications?,"How is federated learning enhancing privacy in healthcare applications? - Machine learning
- Healthcare data
- Data privacy
- Federated learning benefits
- Health informatics
- Patient data privacy","Federated Learning for Healthcare Domain - Pipeline, Applications and
  Challenges","  Federated learning is the process of developing machine learning models over
datasets distributed across data centers such as hospitals, clinical research
labs, and mobile devices while preventing data leakage. This survey examines
previous research and studies on federated learning in the healthcare sector
across a range of use cases and applications. Our survey shows what challenges,
methods, and applications a practitioner should be aware of in the topic of
federated learning. This paper aims to lay out existing research and list the
possibilities of federated learning for healthcare industries.
",8.666666666666666,0.7459022,expanded,6.8,7.333333333333334,4
How is federated learning enhancing privacy in healthcare applications?,"How is federated learning enhancing privacy in healthcare applications? - Machine learning
- Healthcare data
- Data privacy
- Federated learning benefits
- Health informatics
- Patient data privacy",The Future of Digital Health with Federated Learning,"  Data-driven Machine Learning has emerged as a promising approach for building
accurate and robust statistical models from medical data, which is collected in
huge volumes by modern healthcare systems. Existing medical data is not fully
exploited by ML primarily because it sits in data silos and privacy concerns
restrict access to this data. However, without access to sufficient data, ML
will be prevented from reaching its full potential and, ultimately, from making
the transition from research to clinical practice. This paper considers key
factors contributing to this issue, explores how Federated Learning (FL) may
provide a solution for the future of digital health and highlights the
challenges and considerations that need to be addressed.
",7.0,0.6415031,expanded,6.8,7.333333333333334,4
How is federated learning enhancing privacy in healthcare applications?,"How is federated learning enhancing privacy in healthcare applications? - Machine learning
- Healthcare data
- Data privacy
- Federated learning benefits
- Health informatics
- Patient data privacy",Anonymizing Data for Privacy-Preserving Federated Learning,"  Federated learning enables training a global machine learning model from data
distributed across multiple sites, without having to move the data. This is
particularly relevant in healthcare applications, where data is rife with
personal, highly-sensitive information, and data analysis methods must provably
comply with regulatory guidelines. Although federated learning prevents sharing
raw data, it is still possible to launch privacy attacks on the model
parameters that are exposed during the training process, or on the generated
machine learning model. In this paper, we propose the first syntactic approach
for offering privacy in the context of federated learning. Unlike the
state-of-the-art differential privacy-based frameworks, our approach aims to
maximize utility or model performance, while supporting a defensible level of
privacy, as demanded by GDPR and HIPAA. We perform a comprehensive empirical
evaluation on two important problems in the healthcare domain, using real-world
electronic health data of 1 million patients. The results demonstrate the
effectiveness of our approach in achieving high model performance, while
offering the desired level of privacy. Through comparative studies, we also
show that, for varying datasets, experimental setups, and privacy budgets, our
approach offers higher model performance than differential privacy-based
techniques in federated learning.
",9.666666666666666,0.7341206,expanded,6.8,7.333333333333334,4
How is federated learning enhancing privacy in healthcare applications?,"How is federated learning enhancing privacy in healthcare applications? - Machine learning
- Healthcare data
- Data privacy
- Federated learning benefits
- Health informatics
- Patient data privacy","Differential Privacy-enabled Federated Learning for Sensitive Health
  Data","  Leveraging real-world health data for machine learning tasks requires
addressing many practical challenges, such as distributed data silos, privacy
concerns with creating a centralized database from person-specific sensitive
data, resource constraints for transferring and integrating data from multiple
sites, and risk of a single point of failure. In this paper, we introduce a
federated learning framework that can learn a global model from distributed
health data held locally at different sites. The framework offers two levels of
privacy protection. First, it does not move or share raw data across sites or
with a centralized server during the model training process. Second, it uses a
differential privacy mechanism to further protect the model from potential
privacy attacks. We perform a comprehensive evaluation of our approach on two
healthcare applications, using real-world electronic health data of 1 million
patients. We demonstrate the feasibility and effectiveness of the federated
learning framework in offering an elevated level of privacy and maintaining
utility of the global model.
",9.0,0.6716088,expanded,6.8,7.333333333333334,4
What are effective methods for hyperparameter tuning in deep learning?,"What are effective methods for hyperparameter tuning in deep learning? - Grid search
- Random search
- Bayesian optimization
- Gradient-based optimization
- Hyperband
- Genetic algorithms
- Population-based training",Optimizer Benchmarking Needs to Account for Hyperparameter Tuning,"  The performance of optimizers, particularly in deep learning, depends
considerably on their chosen hyperparameter configuration. The efficacy of
optimizers is often studied under near-optimal problem-specific
hyperparameters, and finding these settings may be prohibitively costly for
practitioners. In this work, we argue that a fair assessment of optimizers'
performance must take the computational cost of hyperparameter tuning into
account, i.e., how easy it is to find good hyperparameter configurations using
an automatic hyperparameter search. Evaluating a variety of optimizers on an
extensive set of standard datasets and architectures, our results indicate that
Adam is the most practical solution, particularly in low-budget scenarios.
",8.0,0.7004867,overlap,6.8,7.333333333333334,4
What are effective methods for hyperparameter tuning in deep learning?,"What are effective methods for hyperparameter tuning in deep learning? - Grid search
- Random search
- Bayesian optimization
- Gradient-based optimization
- Hyperband
- Genetic algorithms
- Population-based training","Deep Neural Network Hyperparameter Optimization with Orthogonal Array
  Tuning","  Deep learning algorithms have achieved excellent performance lately in a wide
range of fields (e.g., computer version). However, a severe challenge faced by
deep learning is the high dependency on hyper-parameters. The algorithm results
may fluctuate dramatically under the different configuration of
hyper-parameters. Addressing the above issue, this paper presents an efficient
Orthogonal Array Tuning Method (OATM) for deep learning hyper-parameter tuning.
We describe the OATM approach in five detailed steps and elaborate on it using
two widely used deep neural network structures (Recurrent Neural Networks and
Convolutional Neural Networks). The proposed method is compared to the
state-of-the-art hyper-parameter tuning methods including manually (e.g., grid
search and random search) and automatically (e.g., Bayesian Optimization) ones.
The experiment results state that OATM can significantly save the tuning time
compared to the state-of-the-art methods while preserving the satisfying
performance. The codes are open in GitHub
(https://github.com/xiangzhang1015/OATM)
",9.0,0.7163402,overlap,6.8,7.333333333333334,4
What are effective methods for hyperparameter tuning in deep learning?,"What are effective methods for hyperparameter tuning in deep learning? - Grid search
- Random search
- Bayesian optimization
- Gradient-based optimization
- Hyperband
- Genetic algorithms
- Population-based training","Scalable Gradient-Based Tuning of Continuous Regularization
  Hyperparameters","  Hyperparameter selection generally relies on running multiple full training
trials, with selection based on validation set performance. We propose a
gradient-based approach for locally adjusting hyperparameters during training
of the model. Hyperparameters are adjusted so as to make the model parameter
gradients, and hence updates, more advantageous for the validation cost. We
explore the approach for tuning regularization hyperparameters and find that in
experiments on MNIST, SVHN and CIFAR-10, the resulting regularization levels
are within the optimal regions. The additional computational cost depends on
how frequently the hyperparameters are trained, but the tested scheme adds only
30% computational overhead regardless of the model size. Since the method is
significantly less computationally demanding compared to similar gradient-based
approaches to hyperparameter optimization, and consistently finds good
hyperparameter values, it can be a useful tool for training neural network
models.
",8.333333333333334,0.6990697,original,6.8,7.333333333333334,4
What are effective methods for hyperparameter tuning in deep learning?,"What are effective methods for hyperparameter tuning in deep learning? - Grid search
- Random search
- Bayesian optimization
- Gradient-based optimization
- Hyperband
- Genetic algorithms
- Population-based training",Gradient Descent: The Ultimate Optimizer,"  Working with any gradient-based machine learning algorithm involves the
tedious task of tuning the optimizer's hyperparameters, such as its step size.
Recent work has shown how the step size can itself be optimized alongside the
model parameters by manually deriving expressions for ""hypergradients"" ahead of
time.
  We show how to automatically compute hypergradients with a simple and elegant
modification to backpropagation. This allows us to easily apply the method to
other optimizers and hyperparameters (e.g. momentum coefficients). We can even
recursively apply the method to its own hyper-hyperparameters, and so on ad
infinitum. As these towers of optimizers grow taller, they become less
sensitive to the initial choice of hyperparameters. We present experiments
validating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch
implementation of this algorithm (see
people.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer).
",7.0,0.666735,original,6.8,7.333333333333334,4
What are effective methods for hyperparameter tuning in deep learning?,"What are effective methods for hyperparameter tuning in deep learning? - Grid search
- Random search
- Bayesian optimization
- Gradient-based optimization
- Hyperband
- Genetic algorithms
- Population-based training","High Per Parameter: A Large-Scale Study of Hyperparameter Tuning for
  Machine Learning Algorithms","  Hyperparameters in machine learning (ML) have received a fair amount of
attention, and hyperparameter tuning has come to be regarded as an important
step in the ML pipeline. But just how useful is said tuning? While
smaller-scale experiments have been previously conducted, herein we carry out a
large-scale investigation, specifically, one involving 26 ML algorithms, 250
datasets (regression and both binary and multinomial classification), 6 score
metrics, and 28,857,600 algorithm runs. Analyzing the results we conclude that
for many ML algorithms we should not expect considerable gains from
hyperparameter tuning on average, however, there may be some datasets for which
default hyperparameters perform poorly, this latter being truer for some
algorithms than others. By defining a single hp_score value, which combines an
algorithm's accumulated statistics, we are able to rank the 26 ML algorithms
from those expected to gain the most from hyperparameter tuning to those
expected to gain the least. We believe such a study may serve ML practitioners
at large.
",5.333333333333333,0.66076714,original,6.8,7.333333333333334,4
What are effective methods for hyperparameter tuning in deep learning?,"What are effective methods for hyperparameter tuning in deep learning? - Grid search
- Random search
- Bayesian optimization
- Gradient-based optimization
- Hyperband
- Genetic algorithms
- Population-based training","Combination of Hyperband and Bayesian Optimization for Hyperparameter
  Optimization in Deep Learning","  Deep learning has achieved impressive results on many problems. However, it
requires high degree of expertise or a lot of experience to tune well the
hyperparameters, and such manual tuning process is likely to be biased.
Moreover, it is not practical to try out as many different hyperparameter
configurations in deep learning as in other machine learning scenarios, because
evaluating each single hyperparameter configuration in deep learning would mean
training a deep neural network, which usually takes quite long time. Hyperband
algorithm achieves state-of-the-art performance on various hyperparameter
optimization problems in the field of deep learning. However, Hyperband
algorithm does not utilize history information of previous explored
hyperparameter configurations, thus the solution found is suboptimal. We
propose to combine Hyperband algorithm with Bayesian optimization (which does
not ignore history when sampling next trial configuration). Experimental
results show that our combination approach is superior to other hyperparameter
optimization approaches including Hyperband algorithm.
",9.333333333333334,0.62422997,expanded,6.8,7.333333333333334,4
What are effective methods for hyperparameter tuning in deep learning?,"What are effective methods for hyperparameter tuning in deep learning? - Grid search
- Random search
- Bayesian optimization
- Gradient-based optimization
- Hyperband
- Genetic algorithms
- Population-based training","Automatic Setting of DNN Hyper-Parameters by Mixing Bayesian
  Optimization and Tuning Rules","  Deep learning techniques play an increasingly important role in industrial
and research environments due to their outstanding results. However, the large
number of hyper-parameters to be set may lead to errors if they are set
manually. The state-of-the-art hyper-parameters tuning methods are grid search,
random search, and Bayesian Optimization. The first two methods are expensive
because they try, respectively, all possible combinations and random
combinations of hyper-parameters. Bayesian Optimization, instead, builds a
surrogate model of the objective function, quantifies the uncertainty in the
surrogate using Gaussian Process Regression and uses an acquisition function to
decide where to sample the new set of hyper-parameters. This work faces the
field of Hyper-Parameters Optimization (HPO). The aim is to improve Bayesian
Optimization applied to Deep Neural Networks. For this goal, we build a new
algorithm for evaluating and analyzing the results of the network on the
training and validation sets and use a set of tuning rules to add new
hyper-parameters and/or to reduce the hyper-parameter search space to select a
better combination.
",9.0,0.6554905,expanded,6.8,7.333333333333334,4
What are effective methods for hyperparameter tuning in deep learning?,"What are effective methods for hyperparameter tuning in deep learning? - Grid search
- Random search
- Bayesian optimization
- Gradient-based optimization
- Hyperband
- Genetic algorithms
- Population-based training","A Linear Programming Enhanced Genetic Algorithm for Hyperparameter
  Tuning in Machine Learning","  In this paper, we formulate the hyperparameter tuning problem in machine
learning as a bilevel program. The bilevel program is solved using a micro
genetic algorithm that is enhanced with a linear program. While the genetic
algorithm searches over discrete hyperparameters, the linear program
enhancement allows hyper local search over continuous hyperparameters. The
major contribution in this paper is the formulation of a linear program that
supports fast search over continuous hyperparameters, and can be integrated
with any hyperparameter search technique. It can also be applied directly on
any trained machine learning or deep learning model for the purpose of
fine-tuning. We test the performance of the proposed approach on two datasets,
MNIST and CIFAR-10. Our results clearly demonstrate that using the linear
program enhancement offers significant promise when incorporated with any
population-based approach for hyperparameter tuning.
",7.666666666666667,0.6394814,expanded,6.8,7.333333333333334,4
How can active learning reduce labeling costs in supervised learning?,"How can active learning reduce labeling costs in supervised learning? Semi-supervised learning, label efficiency, data annotation, human-in-the-loop, informative sampling, uncertainty sampling",Active Learning for Cost-Sensitive Classification,"  We design an active learning algorithm for cost-sensitive multiclass
classification: problems where different errors have different costs. Our
algorithm, COAL, makes predictions by regressing to each label's cost and
predicting the smallest. On a new example, it uses a set of regressors that
perform well on past data to estimate possible costs for each label. It queries
only the labels that could be the best, ignoring the sure losers. We prove COAL
can be efficiently implemented for any regression family that admits squared
loss optimization; it also enjoys strong guarantees with respect to predictive
performance and labeling effort. We empirically compare COAL to passive
learning and several active learning baselines, showing significant
improvements in labeling effort and test cost on real-world datasets.
",9.0,0.7681323,original,6.8,7.333333333333334,4
How can active learning reduce labeling costs in supervised learning?,"How can active learning reduce labeling costs in supervised learning? Semi-supervised learning, label efficiency, data annotation, human-in-the-loop, informative sampling, uncertainty sampling",Average-Case Active Learning with Costs,"  We analyze the expected cost of a greedy active learning algorithm. Our
analysis extends previous work to a more general setting in which different
queries have different costs. Moreover, queries may have more than two possible
responses and the distribution over hypotheses may be non uniform. Specific
applications include active learning with label costs, active learning for
multiclass and partial label queries, and batch mode active learning. We also
discuss an approximate version of interest when there are very many queries.
",8.666666666666666,0.77653396,original,6.8,7.333333333333334,4
How can active learning reduce labeling costs in supervised learning?,"How can active learning reduce labeling costs in supervised learning? Semi-supervised learning, label efficiency, data annotation, human-in-the-loop, informative sampling, uncertainty sampling",Active Learning under Label Shift,"  We address the problem of active learning under label shift: when the class
proportions of source and target domains differ. We introduce a ""medial
distribution"" to incorporate a tradeoff between importance weighting and
class-balanced sampling and propose their combined usage in active learning.
Our method is known as Mediated Active Learning under Label Shift (MALLS). It
balances the bias from class-balanced sampling and the variance from importance
weighting. We prove sample complexity and generalization guarantees for MALLS
which show active learning reduces asymptotic sample complexity even under
arbitrary label shift. We empirically demonstrate MALLS scales to
high-dimensional datasets and can reduce the sample complexity of active
learning by 60% in deep active learning tasks.
",8.666666666666666,0.76434976,overlap,6.8,7.333333333333334,4
How can active learning reduce labeling costs in supervised learning?,"How can active learning reduce labeling costs in supervised learning? Semi-supervised learning, label efficiency, data annotation, human-in-the-loop, informative sampling, uncertainty sampling",Compute-Efficient Active Learning,"  Active learning, a powerful paradigm in machine learning, aims at reducing
labeling costs by selecting the most informative samples from an unlabeled
dataset. However, the traditional active learning process often demands
extensive computational resources, hindering scalability and efficiency. In
this paper, we address this critical issue by presenting a novel method
designed to alleviate the computational burden associated with active learning
on massive datasets. To achieve this goal, we introduce a simple, yet effective
method-agnostic framework that outlines how to strategically choose and
annotate data points, optimizing the process for efficiency while maintaining
model performance. Through case studies, we demonstrate the effectiveness of
our proposed method in reducing computational costs while maintaining or, in
some cases, even surpassing baseline model outcomes. Code is available at
https://github.com/aimotive/Compute-Efficient-Active-Learning.
",9.0,0.7757789,overlap,6.8,7.333333333333334,4
How can active learning reduce labeling costs in supervised learning?,"How can active learning reduce labeling costs in supervised learning? Semi-supervised learning, label efficiency, data annotation, human-in-the-loop, informative sampling, uncertainty sampling",Active Learning: Problem Settings and Recent Developments,"  In supervised learning, acquiring labeled training data for a predictive
model can be very costly, but acquiring a large amount of unlabeled data is
often quite easy. Active learning is a method of obtaining predictive models
with high precision at a limited cost through the adaptive selection of samples
for labeling. This paper explains the basic problem settings of active learning
and recent research trends. In particular, research on learning acquisition
functions to select samples from the data for labeling, theoretical work on
active learning algorithms, and stopping criteria for sequential data
acquisition are highlighted. Application examples for material development and
measurement are introduced.
",9.666666666666666,0.7604336,original,6.8,7.333333333333334,4
How can active learning reduce labeling costs in supervised learning?,"How can active learning reduce labeling costs in supervised learning? Semi-supervised learning, label efficiency, data annotation, human-in-the-loop, informative sampling, uncertainty sampling","Consistency-based Semi-supervised Active Learning: Towards Minimizing
  Labeling Cost","  Active learning (AL) combines data labeling and model training to minimize
the labeling cost by prioritizing the selection of high value data that can
best improve model performance. In pool-based active learning, accessible
unlabeled data are not used for model training in most conventional methods.
Here, we propose to unify unlabeled sample selection and model training towards
minimizing labeling cost, and make two contributions towards that end. First,
we exploit both labeled and unlabeled data using semi-supervised learning (SSL)
to distill information from unlabeled data during the training stage. Second,
we propose a consistency-based sample selection metric that is coherent with
the training objective such that the selected samples are effective at
improving model performance. We conduct extensive experiments on image
classification tasks. The experimental results on CIFAR-10, CIFAR-100 and
ImageNet demonstrate the superior performance of our proposed method with
limited labeled data, compared to the existing methods and the alternative AL
and SSL combinations. Additionally, we study an important yet under-explored
problem -- ""When can we start learning-based AL selection?"". We propose a
measure that is empirically correlated with the AL target loss and is
potentially useful for determining the proper starting point of learning-based
AL methods.
",8.666666666666666,0.71542263,expanded,6.8,7.333333333333334,4
How can active learning reduce labeling costs in supervised learning?,"How can active learning reduce labeling costs in supervised learning? Semi-supervised learning, label efficiency, data annotation, human-in-the-loop, informative sampling, uncertainty sampling",Active Learning with Label Comparisons,"  Supervised learning typically relies on manual annotation of the true labels.
When there are many potential classes, searching for the best one can be
prohibitive for a human annotator. On the other hand, comparing two candidate
labels is often much easier. We focus on this type of pairwise supervision and
ask how it can be used effectively in learning, and in particular in active
learning. We obtain several insightful results in this context. In principle,
finding the best of $k$ labels can be done with $k-1$ active queries. We show
that there is a natural class where this approach is sub-optimal, and that
there is a more comparison-efficient active learning scheme. A key element in
our analysis is the ""label neighborhood graph"" of the true distribution, which
has an edge between two classes if they share a decision boundary. We also show
that in the PAC setting, pairwise comparisons cannot provide improved sample
complexity in the worst case. We complement our theoretical results with
experiments, clearly demonstrating the effect of the neighborhood graph on
sample complexity.
",8.0,0.7471113,expanded,6.8,7.333333333333334,4
How can active learning reduce labeling costs in supervised learning?,"How can active learning reduce labeling costs in supervised learning? Semi-supervised learning, label efficiency, data annotation, human-in-the-loop, informative sampling, uncertainty sampling","On the Relationship between Data Efficiency and Error for Uncertainty
  Sampling","  While active learning offers potential cost savings, the actual data
efficiency---the reduction in amount of labeled data needed to obtain the same
error rate---observed in practice is mixed. This paper poses a basic question:
when is active learning actually helpful? We provide an answer for logistic
regression with the popular active learning algorithm, uncertainty sampling.
Empirically, on 21 datasets from OpenML, we find a strong inverse correlation
between data efficiency and the error rate of the final classifier.
Theoretically, we show that for a variant of uncertainty sampling, the
asymptotic data efficiency is within a constant factor of the inverse error
rate of the limiting classifier.
",7.0,0.7375362,expanded,6.8,7.333333333333334,4
What are the advancements in dimensionality reduction techniques for visualization?,"What are the advancements in dimensionality reduction techniques for visualization? - t-SNE
- UMAP
- Autoencoders
- Nonlinear dimensionality reduction
- Data visualization techniques
- Manifold learning","Spectral Overlap and a Comparison of Parameter-Free, Dimensionality
  Reduction Quality Metrics","  Nonlinear dimensionality reduction methods are a popular tool for data
scientists and researchers to visualize complex, high dimensional data.
However, while these methods continue to improve and grow in number, it is
often difficult to evaluate the quality of a visualization due to a variety of
factors such as lack of information about the intrinsic dimension of the data
and additional tuning required for many evaluation metrics. In this paper, we
seek to provide a systematic comparison of dimensionality reduction quality
metrics using datasets where we know the ground truth manifold. We utilize each
metric for hyperparameter optimization in popular dimensionality reduction
methods used for visualization and provide quantitative metrics to objectively
compare visualizations to their original manifold. In our results, we find a
few methods that appear to consistently do well and propose the best performer
as a benchmark for evaluating dimensionality reduction based visualizations.
",6.666666666666667,0.7267655,overlap,6.8,7.333333333333334,4
What are the advancements in dimensionality reduction techniques for visualization?,"What are the advancements in dimensionality reduction techniques for visualization? - t-SNE
- UMAP
- Autoencoders
- Nonlinear dimensionality reduction
- Data visualization techniques
- Manifold learning","A Review, Framework and R toolkit for Exploring, Evaluating, and
  Comparing Visualizations","  This paper gives a review and synthesis of methods of evaluating
dimensionality reduction techniques. Particular attention is paid to rank-order
neighborhood evaluation metrics. A framework is created for exploring
dimensionality reduction quality through visualization. An associated toolkit
is implemented in R. The toolkit includes scatter plots, heat maps, loess
smoothing, and performance lift diagrams. The overall rationale is to help
researchers compare dimensionality reduction techniques and use visual insights
to help select and improve techniques. Examples are given for dimensionality
reduction of manifolds and for the dimensionality reduction applied to a
consumer survey dataset.
",7.0,0.6849644,overlap,6.8,7.333333333333334,4
What are the advancements in dimensionality reduction techniques for visualization?,"What are the advancements in dimensionality reduction techniques for visualization? - t-SNE
- UMAP
- Autoencoders
- Nonlinear dimensionality reduction
- Data visualization techniques
- Manifold learning","A Visual Interaction Framework for Dimensionality Reduction Based Data
  Exploration","  Dimensionality reduction is a common method for analyzing and visualizing
high-dimensional data. However, reasoning dynamically about the results of a
dimensionality reduction is difficult. Dimensionality-reduction algorithms use
complex optimizations to reduce the number of dimensions of a dataset, but
these new dimensions often lack a clear relation to the initial data
dimensions, thus making them difficult to interpret. Here we propose a visual
interaction framework to improve dimensionality-reduction based exploratory
data analysis. We introduce two interaction techniques, forward projection and
backward projection, for dynamically reasoning about dimensionally reduced
data. We also contribute two visualization techniques, prolines and feasibility
maps, to facilitate the effective use of the proposed interactions. We apply
our framework to PCA and autoencoder-based dimensionality reductions. Through
data-exploration examples, we demonstrate how our visual interactions can
improve the use of dimensionality reduction in exploratory data analysis.
",8.0,0.6849685,original,6.8,7.333333333333334,4
What are the advancements in dimensionality reduction techniques for visualization?,"What are the advancements in dimensionality reduction techniques for visualization? - t-SNE
- UMAP
- Autoencoders
- Nonlinear dimensionality reduction
- Data visualization techniques
- Manifold learning","A Spectral Method for Assessing and Combining Multiple Data
  Visualizations","  Dimension reduction and data visualization aim to project a high-dimensional
dataset to a low-dimensional space while capturing the intrinsic structures in
the data. It is an indispensable part of modern data science, and many
dimensional reduction and visualization algorithms have been developed.
However, different algorithms have their own strengths and weaknesses, making
it critically important to evaluate their relative performance for a given
dataset, and to leverage and combine their individual strengths. In this paper,
we propose an efficient spectral method for assessing and combining multiple
visualizations of a given dataset produced by diverse algorithms. The proposed
method provides a quantitative measure -- the visualization eigenscore -- of
the relative performance of the visualizations for preserving the structure
around each data point. Then it leverages the eigenscores to obtain a consensus
visualization, which has much improved { quality over the individual
visualizations in capturing the underlying true data structure.} Our approach
is flexible and works as a wrapper around any visualizations. We analyze
multiple simulated and real-world datasets from diverse applications to
demonstrate the effectiveness of the eigenscores for evaluating visualizations
and the superiority of the proposed consensus visualization. Furthermore, we
establish rigorous theoretical justification of our method based on a general
statistical framework, yielding fundamental principles behind the empirical
success of consensus visualization along with practical guidance.
",7.666666666666667,0.6895957,original,6.8,7.333333333333334,4
What are the advancements in dimensionality reduction techniques for visualization?,"What are the advancements in dimensionality reduction techniques for visualization? - t-SNE
- UMAP
- Autoencoders
- Nonlinear dimensionality reduction
- Data visualization techniques
- Manifold learning","""Why Here and Not There?"" -- Diverse Contrasting Explanations of
  Dimensionality Reduction","  Dimensionality reduction is a popular preprocessing and a widely used tool in
data mining. Transparency, which is usually achieved by means of explanations,
is nowadays a widely accepted and crucial requirement of machine learning based
systems like classifiers and recommender systems. However, transparency of
dimensionality reduction and other data mining tools have not been considered
in much depth yet, still it is crucial to understand their behavior -- in
particular practitioners might want to understand why a specific sample got
mapped to a specific location.
  In order to (locally) understand the behavior of a given dimensionality
reduction method, we introduce the abstract concept of contrasting explanations
for dimensionality reduction, and apply a realization of this concept to the
specific application of explaining two dimensional data visualization.
",7.0,0.6590735,original,6.8,7.333333333333334,4
What are the advancements in dimensionality reduction techniques for visualization?,"What are the advancements in dimensionality reduction techniques for visualization? - t-SNE
- UMAP
- Autoencoders
- Nonlinear dimensionality reduction
- Data visualization techniques
- Manifold learning","UMAP: Uniform Manifold Approximation and Projection for Dimension
  Reduction","  UMAP (Uniform Manifold Approximation and Projection) is a novel manifold
learning technique for dimension reduction. UMAP is constructed from a
theoretical framework based in Riemannian geometry and algebraic topology. The
result is a practical scalable algorithm that applies to real world data. The
UMAP algorithm is competitive with t-SNE for visualization quality, and
arguably preserves more of the global structure with superior run time
performance. Furthermore, UMAP has no computational restrictions on embedding
dimension, making it viable as a general purpose dimension reduction technique
for machine learning.
",9.0,0.5969821,expanded,6.8,7.333333333333334,4
What are the advancements in dimensionality reduction techniques for visualization?,"What are the advancements in dimensionality reduction techniques for visualization? - t-SNE
- UMAP
- Autoencoders
- Nonlinear dimensionality reduction
- Data visualization techniques
- Manifold learning",Multi-view Data Visualisation via Manifold Learning,"  Non-linear dimensionality reduction can be performed by \textit{manifold
learning} approaches, such as Stochastic Neighbour Embedding (SNE), Locally
Linear Embedding (LLE) and Isometric Feature Mapping (ISOMAP). These methods
aim to produce two or three latent embeddings, primarily to visualise the data
in intelligible representations. This manuscript proposes extensions of
Student's t-distributed SNE (t-SNE), LLE and ISOMAP, for dimensionality
reduction and visualisation of multi-view data. Multi-view data refers to
multiple types of data generated from the same samples. The proposed multi-view
approaches provide more comprehensible projections of the samples compared to
the ones obtained by visualising each data-view separately. Commonly
visualisation is used for identifying underlying patterns within the samples.
By incorporating the obtained low-dimensional embeddings from the multi-view
manifold approaches into the K-means clustering algorithm, it is shown that
clusters of the samples are accurately identified. Through the analysis of real
and synthetic data the proposed multi-SNE approach is found to have the best
performance. We further illustrate the applicability of the multi-SNE approach
for the analysis of multi-omics single-cell data, where the aim is to visualise
and identify cell heterogeneity and cell types in biological tissues relevant
to health and disease.
",8.333333333333334,0.57459146,expanded,6.8,7.333333333333334,4
What are the advancements in dimensionality reduction techniques for visualization?,"What are the advancements in dimensionality reduction techniques for visualization? - t-SNE
- UMAP
- Autoencoders
- Nonlinear dimensionality reduction
- Data visualization techniques
- Manifold learning",Modern Dimension Reduction,"  Data are not only ubiquitous in society, but are increasingly complex both in
size and dimensionality. Dimension reduction offers researchers and scholars
the ability to make such complex, high dimensional data spaces simpler and more
manageable. This Element offers readers a suite of modern unsupervised
dimension reduction techniques along with hundreds of lines of R code, to
efficiently represent the original high dimensional data space in a simplified,
lower dimensional subspace. Launching from the earliest dimension reduction
technique principal components analysis and using real social science data, I
introduce and walk readers through application of the following techniques:
locally linear embedding, t-distributed stochastic neighbor embedding (t-SNE),
uniform manifold approximation and projection, self-organizing maps, and deep
autoencoders. The result is a well-stocked toolbox of unsupervised algorithms
for tackling the complexities of high dimensional data so common in modern
society. All code is publicly accessible on Github.
",8.333333333333334,0.587735,expanded,6.8,7.333333333333334,4
How is NLP being used to analyze sentiment in financial news?,"How is NLP being used to analyze sentiment in financial news? - Sentiment analysis
- Natural language processing applications
- Financial sentiment analysis
- Text analytics in finance
- Opinion mining techniques
- Algorithmic trading with NLP",Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts,"  The use of robo-readers to analyze news texts is an emerging technology trend
in computational finance. In recent research, a substantial effort has been
invested to develop sophisticated financial polarity-lexicons that can be used
to investigate how financial sentiments relate to future company performance.
However, based on experience from other fields, where sentiment analysis is
commonly applied, it is well-known that the overall semantic orientation of a
sentence may differ from the prior polarity of individual words. The objective
of this article is to investigate how semantic orientations can be better
detected in financial and economic news by accommodating the overall
phrase-structure information and domain-specific use of language. Our three
main contributions are: (1) establishment of a human-annotated finance
phrase-bank, which can be used as benchmark for training and evaluating
alternative models; (2) presentation of a technique to enhance financial
lexicons with attributes that help to identify expected direction of events
that affect overall sentiment; (3) development of a linearized phrase-structure
model for detecting contextual semantic orientations in financial and economic
news texts. The relevance of the newly added lexicon features and the benefit
of using the proposed learning-algorithm are demonstrated in a comparative
study against previously used general sentiment models as well as the popular
word frequency models used in recent financial studies. The proposed framework
is parsimonious and avoids the explosion in feature-space caused by the use of
conventional n-gram features.
",9.0,0.731992,original,6.8,7.333333333333334,4
How is NLP being used to analyze sentiment in financial news?,"How is NLP being used to analyze sentiment in financial news? - Sentiment analysis
- Natural language processing applications
- Financial sentiment analysis
- Text analytics in finance
- Opinion mining techniques
- Algorithmic trading with NLP","Fine-Tuning Gemma-7B for Enhanced Sentiment Analysis of Financial News
  Headlines","  In this study, we explore the application of sentiment analysis on financial
news headlines to understand investor sentiment. By leveraging Natural Language
Processing (NLP) and Large Language Models (LLM), we analyze sentiment from the
perspective of retail investors. The FinancialPhraseBank dataset, which
contains categorized sentiments of financial news headlines, serves as the
basis for our analysis. We fine-tuned several models, including
distilbert-base-uncased, Llama, and gemma-7b, to evaluate their effectiveness
in sentiment classification. Our experiments demonstrate that the fine-tuned
gemma-7b model outperforms others, achieving the highest precision, recall, and
F1 score. Specifically, the gemma-7b model showed significant improvements in
accuracy after fine-tuning, indicating its robustness in capturing the nuances
of financial sentiment. This model can be instrumental in providing market
insights, risk management, and aiding investment decisions by accurately
predicting the sentiment of financial news. The results highlight the potential
of advanced LLMs in transforming how we analyze and interpret financial
information, offering a powerful tool for stakeholders in the financial
industry.
",9.333333333333334,0.7349585,original,6.8,7.333333333333334,4
How is NLP being used to analyze sentiment in financial news?,"How is NLP being used to analyze sentiment in financial news? - Sentiment analysis
- Natural language processing applications
- Financial sentiment analysis
- Text analytics in finance
- Opinion mining techniques
- Algorithmic trading with NLP","On Quantifying Sentiments of Financial News -- Are We Doing the Right
  Things?","  Typical investors start off the day by going through the daily news to get an
intuition about the performance of the market. The speculations based on the
tone of the news ultimately shape their responses towards the market. Today,
computers are being trained to compute the news sentiment so that it can be
used as a variable to predict stock market movements and returns. Some
researchers have even developed news-based market indices to forecast stock
market returns. Majority of the research in the field of news sentiment
analysis has focussed on using libraries like Vader, Loughran-McDonald (LM),
Harvard IV and Pattern. However, are the popular approaches for measuring
financial news sentiment really approaching the problem of sentiment analysis
correctly? Our experiments suggest that measuring sentiments using these
libraries, especially for financial news, fails to depict the true picture and
hence may not be very reliable. Therefore, the question remains: What is the
most effective and accurate approach to measure financial news sentiment? Our
paper explores these questions and attempts to answer them through SENTInews: a
one-of-its-kind financial news sentiment analyzer customized to the Indian
context
",9.333333333333334,0.7007112,overlap,6.8,7.333333333333334,4
How is NLP being used to analyze sentiment in financial news?,"How is NLP being used to analyze sentiment in financial news? - Sentiment analysis
- Natural language processing applications
- Financial sentiment analysis
- Text analytics in finance
- Opinion mining techniques
- Algorithmic trading with NLP",BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights,"  This paper explores the intersection of Natural Language Processing (NLP) and
financial analysis, focusing on the impact of sentiment analysis in stock price
prediction. We employ BERTopic, an advanced NLP technique, to analyze the
sentiment of topics derived from stock market comments. Our methodology
integrates this sentiment analysis with various deep learning models, renowned
for their effectiveness in time series and stock prediction tasks. Through
comprehensive experiments, we demonstrate that incorporating topic sentiment
notably enhances the performance of these models. The results indicate that
topics in stock market comments provide implicit, valuable insights into stock
market volatility and price trends. This study contributes to the field by
showcasing the potential of NLP in enriching financial analysis and opens up
avenues for further research into real-time sentiment analysis and the
exploration of emotional and contextual aspects of market sentiment. The
integration of advanced NLP techniques like BERTopic with traditional financial
analysis methods marks a step forward in developing more sophisticated tools
for understanding and predicting market behaviors.
",9.666666666666666,0.7196557,overlap,6.8,7.333333333333334,4
How is NLP being used to analyze sentiment in financial news?,"How is NLP being used to analyze sentiment in financial news? - Sentiment analysis
- Natural language processing applications
- Financial sentiment analysis
- Text analytics in finance
- Opinion mining techniques
- Algorithmic trading with NLP",Analyzing FOMC Minutes: Accuracy and Constraints of Language Models,"  This research article analyzes the language used in the official statements
released by the Federal Open Market Committee (FOMC) after its scheduled
meetings to gain insights into the impact of FOMC official statements on
financial markets and economic forecasting. The study reveals that the FOMC is
careful to avoid expressing emotion in their sentences and follows a set of
templates to cover economic situations. The analysis employs advanced language
modeling techniques such as VADER and FinBERT, and a trial test with GPT-4. The
results show that FinBERT outperforms other techniques in predicting negative
sentiment accurately. However, the study also highlights the challenges and
limitations of using current NLP techniques to analyze FOMC texts and suggests
the potential for enhancing language models and exploring alternative
approaches.
",6.666666666666667,0.74056804,original,6.8,7.333333333333334,4
How is NLP being used to analyze sentiment in financial news?,"How is NLP being used to analyze sentiment in financial news? - Sentiment analysis
- Natural language processing applications
- Financial sentiment analysis
- Text analytics in finance
- Opinion mining techniques
- Algorithmic trading with NLP",Financial data analysis application via multi-strategy text processing,"  Maintaining financial system stability is critical to economic development,
and early identification of risks and opportunities is essential. The financial
industry contains a wide variety of data, such as financial statements,
customer information, stock trading data, news, etc. Massive heterogeneous data
calls for intelligent algorithms for machines to process and understand. This
paper mainly focuses on the stock trading data and news about China A-share
companies. We present a financial data analysis application, Financial Quotient
Porter, designed to combine textual and numerical data by using a
multi-strategy data mining approach. Additionally, we present our efforts and
plans in deep learning financial text processing application scenarios using
natural language processing (NLP) and knowledge graph (KG) technologies. Based
on KG technology, risks and opportunities can be identified from heterogeneous
data. NLP technology can be used to extract entities, relations, and events
from unstructured text, and analyze market sentiment. Experimental results show
market sentiments towards a company and an industry, as well as news-level
associations between companies.
",8.333333333333334,0.6563101,expanded,6.8,7.333333333333334,4
How is NLP being used to analyze sentiment in financial news?,"How is NLP being used to analyze sentiment in financial news? - Sentiment analysis
- Natural language processing applications
- Financial sentiment analysis
- Text analytics in finance
- Opinion mining techniques
- Algorithmic trading with NLP","Sentiment Analysis of Financial News Articles using Performance
  Indicators","  Mining financial text documents and understanding the sentiments of
individual investors, institutions and markets is an important and challenging
problem in the literature. Current approaches to mine sentiments from financial
texts largely rely on domain specific dictionaries. However, dictionary based
methods often fail to accurately predict the polarity of financial texts. This
paper aims to improve the state-of-the-art and introduces a novel sentiment
analysis approach that employs the concept of financial and non-financial
performance indicators. It presents an association rule mining based
hierarchical sentiment classifier model to predict the polarity of financial
texts as positive, neutral or negative. The performance of the proposed model
is evaluated on a benchmark financial dataset. The model is also compared
against other state-of-the-art dictionary and machine learning based approaches
and the results are found to be quite promising. The novel use of performance
indicators for financial sentiment analysis offers interesting and useful
insights.
",9.0,0.6555717,expanded,6.8,7.333333333333334,4
How is NLP being used to analyze sentiment in financial news?,"How is NLP being used to analyze sentiment in financial news? - Sentiment analysis
- Natural language processing applications
- Financial sentiment analysis
- Text analytics in finance
- Opinion mining techniques
- Algorithmic trading with NLP",Towards Financial Sentiment Analysis in a South African Landscape,"  Sentiment analysis as a sub-field of natural language processing has received
increased attention in the past decade enabling organisations to more
effectively manage their reputation through online media monitoring. Many
drivers impact reputation, however, this thesis focuses only the aspect of
financial performance and explores the gap with regards to financial sentiment
analysis in a South African context. Results showed that pre-trained sentiment
analysers are least effective for this task and that traditional lexicon-based
and machine learning approaches are best suited to predict financial sentiment
of news articles. The evaluated methods produced accuracies of 84\%-94\%. The
predicted sentiments correlated quite well with share price and highlighted the
potential use of sentiment as an indicator of financial performance. A main
contribution of the study was updating an existing sentiment dictionary for
financial sentiment analysis. Model generalisation was less acceptable due to
the limited amount of training data used. Future work includes expanding the
data set to improve general usability and contribute to an open-source
financial sentiment analyser for South African data.
",8.333333333333334,0.662918,expanded,6.8,7.333333333333334,4
What methods are effective for handling missing data in datasets?,"What methods are effective for handling missing data in datasets? - Imputation techniques
- Data preprocessing
- Handling missing values
- Statistical methods
- Machine learning algorithms",Review for Handling Missing Data with special missing mechanism,"  Missing data poses a significant challenge in data science, affecting
decision-making processes and outcomes. Understanding what missing data is, how
it occurs, and why it is crucial to handle it appropriately is paramount when
working with real-world data, especially in tabular data, one of the most
commonly used data types in the real world. Three missing mechanisms are
defined in the literature: Missing Completely At Random (MCAR), Missing At
Random (MAR), and Missing Not At Random (MNAR), each presenting unique
challenges in imputation. Most existing work are focused on MCAR that is
relatively easy to handle. The special missing mechanisms of MNAR and MAR are
less explored and understood. This article reviews existing literature on
handling missing values. It compares and contrasts existing methods in terms of
their ability to handle different missing mechanisms and data types. It
identifies research gap in the existing literature and lays out potential
directions for future research in the field. The information in this review
will help data analysts and researchers to adopt and promote good practices for
handling missing data in real-world problems.
",9.0,0.7181142,original,6.8,7.333333333333334,4
What methods are effective for handling missing data in datasets?,"What methods are effective for handling missing data in datasets? - Imputation techniques
- Data preprocessing
- Handling missing values
- Statistical methods
- Machine learning algorithms",Missing Data Imputation using Optimal Transport,"  Missing data is a crucial issue when applying machine learning algorithms to
real-world datasets. Starting from the simple assumption that two batches
extracted randomly from the same dataset should share the same distribution, we
leverage optimal transport distances to quantify that criterion and turn it
into a loss function to impute missing data values. We propose practical
methods to minimize these losses using end-to-end learning, that can exploit or
not parametric assumptions on the underlying distributions of values. We
evaluate our methods on datasets from the UCI repository, in MCAR, MAR and MNAR
settings. These experiments show that OT-based methods match or out-perform
state-of-the-art imputation methods, even for high percentages of missing
values.
",9.0,0.655189,overlap,6.8,7.333333333333334,4
What methods are effective for handling missing data in datasets?,"What methods are effective for handling missing data in datasets? - Imputation techniques
- Data preprocessing
- Handling missing values
- Statistical methods
- Machine learning algorithms",The Missing Indicator Method: From Low to High Dimensions,"  Missing data is common in applied data science, particularly for tabular data
sets found in healthcare, social sciences, and natural sciences. Most
supervised learning methods only work on complete data, thus requiring
preprocessing such as missing value imputation to work on incomplete data sets.
However, imputation alone does not encode useful information about the missing
values themselves. For data sets with informative missing patterns, the Missing
Indicator Method (MIM), which adds indicator variables to indicate the missing
pattern, can be used in conjunction with imputation to improve model
performance. While commonly used in data science, MIM is surprisingly
understudied from an empirical and especially theoretical perspective. In this
paper, we show empirically and theoretically that MIM improves performance for
informative missing values, and we prove that MIM does not hurt linear models
asymptotically for uninformative missing values. Additionally, we find that for
high-dimensional data sets with many uninformative indicators, MIM can induce
model overfitting and thus test performance. To address this issue, we
introduce Selective MIM (SMIM), a novel MIM extension that adds missing
indicators only for features that have informative missing patterns. We show
empirically that SMIM performs at least as well as MIM in general, and improves
MIM for high-dimensional data. Lastly, to demonstrate the utility of MIM on
real-world data science tasks, we demonstrate the effectiveness of MIM and SMIM
on clinical tasks generated from the MIMIC-III database of electronic health
records.
",8.666666666666666,0.6616597,original,6.8,7.333333333333334,4
What methods are effective for handling missing data in datasets?,"What methods are effective for handling missing data in datasets? - Imputation techniques
- Data preprocessing
- Handling missing values
- Statistical methods
- Machine learning algorithms","A computational study on imputation methods for missing environmental
  data","  Data acquisition and recording in the form of databases are routine
operations. The process of collecting data, however, may experience
irregularities, resulting in databases with missing data. Missing entries might
alter analysis efficiency and, consequently, the associated decision-making
process. This paper focuses on databases collecting information related to the
natural environment. Given the broad spectrum of recorded activities, these
databases typically are of mixed nature. It is therefore relevant to evaluate
the performance of missing data processing methods considering this
characteristic. In this paper we investigate the performances of several
missing data imputation methods and their application to the problem of missing
data in environment. A computational study was performed to compare the method
missForest (MF) with two other imputation methods, namely Multivariate
Imputation by Chained Equations (MICE) and K-Nearest Neighbors (KNN). Tests
were made on 10 pretreated datasets of various types. Results revealed that MF
generally outperformed MICE and KNN in terms of imputation errors, with a more
pronounced performance gap for mixed typed databases where MF reduced the
imputation error up to 150%, when compared to the other methods. KNN was
usually the fastest method. MF was then successfully applied to a case study on
Quebec wastewater treatment plants performance monitoring. We believe that the
present study demonstrates the pertinence of using MF as imputation method when
dealing with missing environmental data.
",9.0,0.64936626,overlap,6.8,7.333333333333334,4
What methods are effective for handling missing data in datasets?,"What methods are effective for handling missing data in datasets? - Imputation techniques
- Data preprocessing
- Handling missing values
- Statistical methods
- Machine learning algorithms","Identification and Estimation for Nonignorable Missing Data: A Data
  Fusion Approach","  We consider the task of identifying and estimating a parameter of interest in
settings where data is missing not at random (MNAR). In general, such
parameters are not identified without strong assumptions on the missing data
model. In this paper, we take an alternative approach and introduce a method
inspired by data fusion, where information in an MNAR dataset is augmented by
information in an auxiliary dataset subject to missingness at random (MAR). We
show that even if the parameter of interest cannot be identified given either
dataset alone, it can be identified given pooled data, under two complementary
sets of assumptions. We derive an inverse probability weighted (IPW) estimator
for identified parameters, and evaluate the performance of our estimation
strategies via simulation studies, and a data application.
",9.333333333333334,0.623152,original,6.8,7.333333333333334,4
What methods are effective for handling missing data in datasets?,"What methods are effective for handling missing data in datasets? - Imputation techniques
- Data preprocessing
- Handling missing values
- Statistical methods
- Machine learning algorithms","Performance comparison of State-of-the-art Missing Value Imputation
  Algorithms on Some Bench mark Datasets","  Decision making from data involves identifying a set of attributes that
contribute to effective decision making through computational intelligence. The
presence of missing values greatly influences the selection of right set of
attributes and this renders degradation in classification accuracies of the
classifiers. As missing values are quite common in data collection phase during
field experiments or clinical trails appropriate handling would improve the
classifier performance. In this paper we present a review of recently developed
missing value imputation algorithms and compare their performance on some bench
mark datasets.
",8.666666666666666,0.5912547,expanded,6.8,7.333333333333334,4
What methods are effective for handling missing data in datasets?,"What methods are effective for handling missing data in datasets? - Imputation techniques
- Data preprocessing
- Handling missing values
- Statistical methods
- Machine learning algorithms","On the Performance of Imputation Techniques for Missing Values on
  Healthcare Datasets","  Missing values or data is one popular characteristic of real-world datasets,
especially healthcare data. This could be frustrating when using machine
learning algorithms on such datasets, simply because most machine learning
models perform poorly in the presence of missing values. The aim of this study
is to compare the performance of seven imputation techniques, namely Mean
imputation, Median Imputation, Last Observation carried Forward (LOCF)
imputation, K-Nearest Neighbor (KNN) imputation, Interpolation imputation,
Missforest imputation, and Multiple imputation by Chained Equations (MICE), on
three healthcare datasets. Some percentage of missing values - 10\%, 15\%, 20\%
and 25\% - were introduced into the dataset, and the imputation techniques were
employed to impute these missing values. The comparison of their performance
was evaluated by using root mean squared error (RMSE) and mean absolute error
(MAE). The results show that Missforest imputation performs the best followed
by MICE imputation. Additionally, we try to determine whether it is better to
perform feature selection before imputation or vice versa by using the
following metrics - the recall, precision, f1-score and accuracy. Due to the
fact that there are few literature on this and some debate on the subject among
researchers, we hope that the results from this experiment will encourage data
scientists and researchers to perform imputation first before feature selection
when dealing with data containing missing values.
",9.333333333333334,0.56054455,expanded,6.8,7.333333333333334,4
What methods are effective for handling missing data in datasets?,"What methods are effective for handling missing data in datasets? - Imputation techniques
- Data preprocessing
- Handling missing values
- Statistical methods
- Machine learning algorithms",Explainable Data Imputation using Constraints,"  Data values in a dataset can be missing or anomalous due to mishandling or
human error. Analysing data with missing values can create bias and affect the
inferences. Several analysis methods, such as principle components analysis or
singular value decomposition, require complete data. Many approaches impute
numeric data and some do not consider dependency of attributes on other
attributes, while some require human intervention and domain knowledge. We
present a new algorithm for data imputation based on different data type values
and their association constraints in data, which are not handled currently by
any system. We show experimental results using different metrics comparing our
algorithm with state of the art imputation techniques. Our algorithm not only
imputes the missing values but also generates human readable explanations
describing the significance of attributes used for every imputation.
",9.0,0.6870532,expanded,6.8,7.333333333333334,4
How is transfer learning improving outcomes in drug discovery research?,"How is transfer learning improving outcomes in drug discovery research? - Machine learning
- Deep learning
- Neural networks
- Bioinformatics
- Pharmacology
- Drug development
- Computational biology
- Transfer learning in healthcare","What Makes Transfer Learning Work For Medical Images: Feature Reuse &
  Other Factors","  Transfer learning is a standard technique to transfer knowledge from one
domain to another. For applications in medical imaging, transfer from ImageNet
has become the de-facto approach, despite differences in the tasks and image
characteristics between the domains. However, it is unclear what factors
determine whether - and to what extent - transfer learning to the medical
domain is useful. The long-standing assumption that features from the source
domain get reused has recently been called into question. Through a series of
experiments on several medical image benchmark datasets, we explore the
relationship between transfer learning, data size, the capacity and inductive
bias of the model, as well as the distance between the source and target
domain. Our findings suggest that transfer learning is beneficial in most
cases, and we characterize the important role feature reuse plays in its
success.
",6.333333333333333,0.5968696,original,6.8,7.333333333333334,4
How is transfer learning improving outcomes in drug discovery research?,"How is transfer learning improving outcomes in drug discovery research? - Machine learning
- Deep learning
- Neural networks
- Bioinformatics
- Pharmacology
- Drug development
- Computational biology
- Transfer learning in healthcare","Research Frontiers in Transfer Learning -- a systematic and bibliometric
  review","  Humans can learn from very few samples, demonstrating an outstanding
generalization ability that learning algorithms are still far from reaching.
Currently, the most successful models demand enormous amounts of well-labeled
data, which are expensive and difficult to obtain, becoming one of the biggest
obstacles to the use of machine learning in practice. This scenario shows the
massive potential for Transfer Learning, which aims to harness previously
acquired knowledge to the learning of new tasks more effectively and
efficiently. In this systematic review, we apply a quantitative method to
select the main contributions to the field and make use of bibliographic
coupling metrics to identify research frontiers. We further analyze the
linguistic variation between the classics of the field and the frontier and map
promising research directions.
",4.0,0.50629115,original,6.8,7.333333333333334,4
How is transfer learning improving outcomes in drug discovery research?,"How is transfer learning improving outcomes in drug discovery research? - Machine learning
- Deep learning
- Neural networks
- Bioinformatics
- Pharmacology
- Drug development
- Computational biology
- Transfer learning in healthcare","Transfer Learning Bayesian Optimization to Design Competitor DNA
  Molecules for Use in Diagnostic Assays","  With the rise in engineered biomolecular devices, there is an increased need
for tailor-made biological sequences. Often, many similar biological sequences
need to be made for a specific application meaning numerous, sometimes
prohibitively expensive, lab experiments are necessary for their optimization.
This paper presents a transfer learning design of experiments workflow to make
this development feasible. By combining a transfer learning surrogate model
with Bayesian optimization, we show how the total number of experiments can be
reduced by sharing information between optimization tasks. We demonstrate the
reduction in the number of experiments using data from the development of DNA
competitors for use in an amplification-based diagnostic assay. We use
cross-validation to compare the predictive accuracy of different transfer
learning models, and then compare the performance of the models for both single
objective and penalized optimization tasks.
",6.333333333333333,0.58544827,original,6.8,7.333333333333334,4
How is transfer learning improving outcomes in drug discovery research?,"How is transfer learning improving outcomes in drug discovery research? - Machine learning
- Deep learning
- Neural networks
- Bioinformatics
- Pharmacology
- Drug development
- Computational biology
- Transfer learning in healthcare",Is Meta-training Really Necessary for Molecular Few-Shot Learning ?,"  Few-shot learning has recently attracted significant interest in drug
discovery, with a recent, fast-growing literature mostly involving convoluted
meta-learning strategies. We revisit the more straightforward fine-tuning
approach for molecular data, and propose a regularized quadratic-probe loss
based on the the Mahalanobis distance. We design a dedicated block-coordinate
descent optimizer, which avoid the degenerate solutions of our loss.
Interestingly, our simple fine-tuning approach achieves highly competitive
performances in comparison to state-of-the-art methods, while being applicable
to black-box settings and removing the need for specific episodic pre-training
strategies. Furthermore, we introduce a new benchmark to assess the robustness
of the competing methods to domain shifts. In this setting, our fine-tuning
baseline obtains consistently better results than meta-learning methods.
",6.333333333333333,0.57634866,original,6.8,7.333333333333334,4
How is transfer learning improving outcomes in drug discovery research?,"How is transfer learning improving outcomes in drug discovery research? - Machine learning
- Deep learning
- Neural networks
- Bioinformatics
- Pharmacology
- Drug development
- Computational biology
- Transfer learning in healthcare","An Integrated Transfer Learning and Multitask Learning Approach for
  Pharmacokinetic Parameter Prediction","  Background: Pharmacokinetic evaluation is one of the key processes in drug
discovery and development. However, current absorption, distribution,
metabolism, excretion prediction models still have limited accuracy. Aim: This
study aims to construct an integrated transfer learning and multitask learning
approach for developing quantitative structure-activity relationship models to
predict four human pharmacokinetic parameters. Methods: A pharmacokinetic
dataset included 1104 U.S. FDA approved small molecule drugs. The dataset
included four human pharmacokinetic parameter subsets (oral bioavailability,
plasma protein binding rate, apparent volume of distribution at steady-state
and elimination half-life). The pre-trained model was trained on over 30
million bioactivity data. An integrated transfer learning and multitask
learning approach was established to enhance the model generalization. Results:
The pharmacokinetic dataset was split into three parts (60:20:20) for training,
validation and test by the improved Maximum Dissimilarity algorithm with the
representative initial set selection algorithm and the weighted distance
function. The multitask learning techniques enhanced the model predictive
ability. The integrated transfer learning and multitask learning model
demonstrated the best accuracies, because deep neural networks have the general
feature extraction ability, transfer learning and multitask learning improved
the model generalization. Conclusions: The integrated transfer learning and
multitask learning approach with the improved dataset splitting algorithm was
firstly introduced to predict the pharmacokinetic parameters. This method can
be further employed in drug discovery and development.
",9.0,0.54675496,overlap,6.8,7.333333333333334,4
How is transfer learning improving outcomes in drug discovery research?,"How is transfer learning improving outcomes in drug discovery research? - Machine learning
- Deep learning
- Neural networks
- Bioinformatics
- Pharmacology
- Drug development
- Computational biology
- Transfer learning in healthcare",Artificial Intelligence for Drug Discovery: Are We There Yet?,"  Drug discovery is adapting to novel technologies such as data science,
informatics, and artificial intelligence (AI) to accelerate effective treatment
development while reducing costs and animal experiments. AI is transforming
drug discovery, as indicated by increasing interest from investors, industrial
and academic scientists, and legislators. Successful drug discovery requires
optimizing properties related to pharmacodynamics, pharmacokinetics, and
clinical outcomes. This review discusses the use of AI in the three pillars of
drug discovery: diseases, targets, and therapeutic modalities, with a focus on
small molecule drugs. AI technologies, such as generative chemistry, machine
learning, and multi-property optimization, have enabled several compounds to
enter clinical trials. The scientific community must carefully vet known
information to address the reproducibility crisis. The full potential of AI in
drug discovery can only be realized with sufficient ground truth and
appropriate human intervention at later pipeline stages.
",5.666666666666667,0.54778695,expanded,6.8,7.333333333333334,4
How is transfer learning improving outcomes in drug discovery research?,"How is transfer learning improving outcomes in drug discovery research? - Machine learning
- Deep learning
- Neural networks
- Bioinformatics
- Pharmacology
- Drug development
- Computational biology
- Transfer learning in healthcare","Applications of artificial intelligence in drug development using
  real-world data","  The US Food and Drug Administration (FDA) has been actively promoting the use
of real-world data (RWD) in drug development. RWD can generate important
real-world evidence reflecting the real-world clinical environment where the
treatments are used. Meanwhile, artificial intelligence (AI), especially
machine- and deep-learning (ML/DL) methods, have been increasingly used across
many stages of the drug development process. Advancements in AI have also
provided new strategies to analyze large, multidimensional RWD. Thus, we
conducted a rapid review of articles from the past 20 years, to provide an
overview of the drug development studies that use both AI and RWD. We found
that the most popular applications were adverse event detection, trial
recruitment, and drug repurposing. Here, we also discuss current research gaps
and future opportunities.
",4.0,0.48406962,expanded,6.8,7.333333333333334,4
How is transfer learning improving outcomes in drug discovery research?,"How is transfer learning improving outcomes in drug discovery research? - Machine learning
- Deep learning
- Neural networks
- Bioinformatics
- Pharmacology
- Drug development
- Computational biology
- Transfer learning in healthcare","Ensemble Transfer Learning for the Prediction of Anti-Cancer Drug
  Response","  Transfer learning has been shown to be effective in many applications in
which training data for the target problem are limited but data for a related
(source) problem are abundant. In this paper, we apply transfer learning to the
prediction of anti-cancer drug response. Previous transfer learning studies for
drug response prediction focused on building models that predict the response
of tumor cells to a specific drug treatment. We target the more challenging
task of building general prediction models that can make predictions for both
new tumor cells and new drugs. We apply the classic transfer learning framework
that trains a prediction model on the source dataset and refines it on the
target dataset, and extends the framework through ensemble. The ensemble
transfer learning pipeline is implemented using LightGBM and two deep neural
network (DNN) models with different architectures. Uniquely, we investigate its
power for three application settings including drug repurposing, precision
oncology, and new drug development, through different data partition schemes in
cross-validation. We test the proposed ensemble transfer learning on benchmark
in vitro drug screening datasets, taking one dataset as the source domain and
another dataset as the target domain. The analysis results demonstrate the
benefit of applying ensemble transfer learning for predicting anti-cancer drug
response in all three applications with both LightGBM and DNN models. Compared
between the different prediction models, a DNN model with two subnetworks for
the inputs of tumor features and drug features separately outperforms LightGBM
and the other DNN model that concatenates tumor features and drug features for
input in the drug repurposing and precision oncology applications. In the more
challenging application of new drug development, LightGBM performs better than
the other two DNN models.
",9.0,0.51386654,expanded,6.8,7.333333333333334,4
How is transfer learning improving outcomes in drug discovery research?,"How is transfer learning improving outcomes in drug discovery research? - Machine learning
- Deep learning
- Neural networks
- Bioinformatics
- Pharmacology
- Drug development
- Computational biology
- Transfer learning in healthcare","Improving Compound Activity Classification via Deep Transfer and
  Representation Learning","  Recent advances in molecular machine learning, especially deep neural
networks such as Graph Neural Networks (GNNs) for predicting structure activity
relationships (SAR) have shown tremendous potential in computer-aided drug
discovery. However, the applicability of such deep neural networks are limited
by the requirement of large amounts of training data. In order to cope with
limited training data for a target task, transfer learning for SAR modeling has
been recently adopted to leverage information from data of related tasks. In
this work, in contrast to the popular parameter-based transfer learning such as
pretraining, we develop novel deep transfer learning methods TAc and TAc-fc to
leverage source domain data and transfer useful information to the target
domain. TAc learns to generate effective molecular features that can generalize
well from one domain to another, and increase the classification performance in
the target domain. Additionally, TAc-fc extends TAc by incorporating novel
components to selectively learn feature-wise and compound-wise transferability.
We used the bioassay screening data from PubChem, and identified 120 pairs of
bioassays such that the active compounds in each pair are more similar to each
other compared to its inactive compounds. Our experiments clearly demonstrate
that TAc achieves significant improvement over all baselines across a large
number of target tasks. Furthermore, although TAc-fc achieves slightly worse
ROC-AUC on average compared to TAc, TAc-fc still achieves the best performance
on more tasks in terms of PR-AUC and F1 compared to other methods. In summary,
TAc-fc is also found to be a strong model with competitive or even better
performance than TAc on a notable number of target tasks.
",9.0,0.56423235,expanded,6.8,7.333333333333334,4
What are the latest machine learning techniques for predictive maintenance?,"What are the latest machine learning techniques for predictive maintenance? - Anomaly detection
- Feature engineering
- Time series analysis
- Supervised learning
- Unsupervised learning
- Deep learning
- Predictive modeling
- Sensor data
- Maintenance optimization",A Benchmark dataset for predictive maintenance,"  The paper describes the MetroPT data set, an outcome of a eXplainable
Predictive Maintenance (XPM) project with an urban metro public transportation
service in Porto, Portugal. The data was collected in 2022 that aimed to
evaluate machine learning methods for online anomaly detection and failure
prediction. By capturing several analogic sensor signals (pressure,
temperature, current consumption), digital signals (control signals, discrete
signals), and GPS information (latitude, longitude, and speed), we provide a
dataset that can be easily used to evaluate online machine learning methods.
This dataset contains some interesting characteristics and can be a good
benchmark for predictive maintenance models.
",5.666666666666667,0.749577,overlap,6.8,7.333333333333334,4
What are the latest machine learning techniques for predictive maintenance?,"What are the latest machine learning techniques for predictive maintenance? - Anomaly detection
- Feature engineering
- Time series analysis
- Supervised learning
- Unsupervised learning
- Deep learning
- Predictive modeling
- Sensor data
- Maintenance optimization",An Economic Perspective on Predictive Maintenance of Filtration Units,"  This paper provides an economic perspective on the predictive maintenance of
filtration units. The rise of predictive maintenance is possible due to the
growing trend of industry 4.0 and the availability of inexpensive sensors.
However, the adoption rate for predictive maintenance by companies remains low.
The majority of companies are sticking to corrective and preventive
maintenance. This is not due to a lack of information on the technical
implementation of predictive maintenance, with an abundance of research papers
on state-of-the-art machine learning algorithms that can be used effectively.
The main issue is that most upper management has not yet been fully convinced
of the idea of predictive maintenance. The economic value of the implementation
has to be linked to the predictive maintenance program for better justification
by the management. In this study, three machine learning models were trained to
demonstrate the economic value of predictive maintenance. Data was collected
from a testbed located at the Singapore University of Technology and Design.
The testbed closely resembles a real-world water treatment plant. A
cost-benefit analysis coupled with Monte Carlo simulation was proposed. It
provided a structured approach to document potential costs and savings by
implementing a predictive maintenance program. The simulation incorporated
real-world risk into a financial model. Financial figures were adapted from
CITIC Envirotech Ltd, a leading membrane-based integrated environmental
solutions provider. Two scenarios were used to elaborate on the economic values
of predictive maintenance. Overall, this study seeks to bridge the gap between
technical and business domains of predictive maintenance.
",4.666666666666667,0.7247919,original,6.8,7.333333333333334,4
What are the latest machine learning techniques for predictive maintenance?,"What are the latest machine learning techniques for predictive maintenance? - Anomaly detection
- Feature engineering
- Time series analysis
- Supervised learning
- Unsupervised learning
- Deep learning
- Predictive modeling
- Sensor data
- Maintenance optimization",Predictive Maintenance using Machine Learning,"  Predictive maintenance (PdM) is a concept, which is implemented to
effectively manage maintenance plans of the assets by predicting their failures
with data driven techniques. In these scenarios, data is collected over a
certain period of time to monitor the state of equipment. The objective is to
find some correlations and patterns that can help predict and ultimately
prevent failures. Equipment in manufacturing industry are often utilized
without a planned maintenance approach. Such practise frequently results in
unexpected downtime, owing to certain unexpected failures. In scheduled
maintenance, the condition of the manufacturing equipment is checked after
fixed time interval and if any fault occurs, the component is replaced to avoid
unexpected equipment stoppages. On the flip side, this leads to increase in
time for which machine is non-functioning and cost of carrying out the
maintenance. The emergence of Industry 4.0 and smart systems have led to
increasing emphasis on predictive maintenance (PdM) strategies that can reduce
the cost of downtime and increase the availability (utilization rate) of
manufacturing equipment. PdM also has the potential to bring about new
sustainable practices in manufacturing by fully utilizing the useful lives of
components.
",6.666666666666667,0.69223505,overlap,6.8,7.333333333333334,4
What are the latest machine learning techniques for predictive maintenance?,"What are the latest machine learning techniques for predictive maintenance? - Anomaly detection
- Feature engineering
- Time series analysis
- Supervised learning
- Unsupervised learning
- Deep learning
- Predictive modeling
- Sensor data
- Maintenance optimization","Revolutionizing System Reliability: The Role of AI in Predictive
  Maintenance Strategies","  The landscape of maintenance in distributed systems is rapidly evolving with
the integration of Artificial Intelligence (AI). Also, as the complexity of
computing continuum systems intensifies, the role of AI in predictive
maintenance (Pd.M.) becomes increasingly pivotal. This paper presents a
comprehensive survey of the current state of Pd.M. in the computing continuum,
with a focus on the combination of scalable AI technologies. Recognizing the
limitations of traditional maintenance practices in the face of increasingly
complex and heterogenous computing continuum systems, the study explores how
AI, especially machine learning and neural networks, is being used to enhance
Pd.M. strategies. The survey encompasses a thorough review of existing
literature, highlighting key advancements, methodologies, and case studies in
the field. It critically examines the role of AI in improving prediction
accuracy for system failures and in optimizing maintenance schedules, thereby
contributing to reduced downtime and enhanced system longevity. By synthesizing
findings from the latest advancements in the field, the article provides
insights into the effectiveness and challenges of implementing AI-driven
predictive maintenance. It underscores the evolution of maintenance practices
in response to technological advancements and the growing complexity of
computing continuum systems. The conclusions drawn from this survey are
instrumental for practitioners and researchers in understanding the current
landscape and future directions of Pd.M. in distributed systems. It emphasizes
the need for continued research and development in this area, pointing towards
a trend of more intelligent, efficient, and cost-effective maintenance
solutions in the era of AI.
",9.0,0.72196645,overlap,6.8,7.333333333333334,4
What are the latest machine learning techniques for predictive maintenance?,"What are the latest machine learning techniques for predictive maintenance? - Anomaly detection
- Feature engineering
- Time series analysis
- Supervised learning
- Unsupervised learning
- Deep learning
- Predictive modeling
- Sensor data
- Maintenance optimization","Comprehensive Study Of Predictive Maintenance In Industries Using
  Classification Models And LSTM Model","  In today's technology-driven era, the imperative for predictive maintenance
and advanced diagnostics extends beyond aviation to encompass the
identification of damages, failures, and operational defects in rotating and
moving machines. Implementing such services not only curtails maintenance costs
but also extends machine lifespan, ensuring heightened operational efficiency.
Moreover, it serves as a preventive measure against potential accidents or
catastrophic events. The advent of Artificial Intelligence (AI) has
revolutionized maintenance across industries, enabling more accurate and
efficient prediction and analysis of machine failures, thereby conserving time
and resources. Our proposed study aims to delve into various machine learning
classification techniques, including Support Vector Machine (SVM), Random
Forest, Logistic Regression, and Convolutional Neural Network LSTM-Based, for
predicting and analyzing machine performance. SVM classifies data into
different categories based on their positions in a multidimensional space,
while Random Forest employs ensemble learning to create multiple decision trees
for classification. Logistic Regression predicts the probability of binary
outcomes using input data. The primary objective of the study is to assess
these algorithms' performance in predicting and analyzing machine performance,
considering factors such as accuracy, precision, recall, and F1 score. The
findings will aid maintenance experts in selecting the most suitable machine
learning algorithm for effective prediction and analysis of machine
performance.
",8.0,0.6876368,overlap,6.8,7.333333333333334,4
What are the latest machine learning techniques for predictive maintenance?,"What are the latest machine learning techniques for predictive maintenance? - Anomaly detection
- Feature engineering
- Time series analysis
- Supervised learning
- Unsupervised learning
- Deep learning
- Predictive modeling
- Sensor data
- Maintenance optimization","Deep learning models for predictive maintenance: a survey, comparison,
  challenges and prospect","  Given the growing amount of industrial data spaces worldwide, deep learning
solutions have become popular for predictive maintenance, which monitor assets
to optimise maintenance tasks. Choosing the most suitable architecture for each
use-case is complex given the number of examples found in literature. This work
aims at facilitating this task by reviewing state-of-the-art deep learning
architectures, and how they integrate with predictive maintenance stages to
meet industrial companies' requirements (i.e. anomaly detection, root cause
analysis, remaining useful life estimation). They are categorised and compared
in industrial applications, explaining how to fill their gaps. Finally, open
challenges and future research paths are presented.
",9.333333333333334,0.68922913,expanded,6.8,7.333333333333334,4
