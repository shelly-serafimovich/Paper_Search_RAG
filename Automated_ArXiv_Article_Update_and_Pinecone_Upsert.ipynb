{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Notebook Description: Automated ArXiv Article Update and Pinecone Upsert**\n",
        "\n",
        "This notebook automates the retrieval, filtering, and upsert of new AI-related articles from the ArXiv dataset into a Pinecone vector index. Starting with a download of the dataset, it filters for recent articles within AI-related categories and encodes each articleâ€™s title and abstract into embeddings. The notebook compares each article's update date against the last stored update to avoid duplication. New articles are then upserted to Pinecone in batches for efficient indexing, with relevant metadata included. Finally, it updates the last processed date, ensuring that future runs only consider articles added after the most recent update. This workflow keeps the Pinecone index current with the latest AI research."
      ],
      "metadata": {
        "id": "C-cnRBBXFZ4O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "098eds01EZZ3"
      },
      "outputs": [],
      "source": [
        "#Imports and installations\n",
        "!pip install kaggle sentence_transformers pinecone\n",
        "!kaggle datasets download -d Cornell-University/arxiv -p /content/dataset --unzip\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from datetime import datetime\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone connection\n",
        "pc = Pinecone(api_key=\"65adfe61-8c99-4c68-951e-e2d42e7884df\")\n",
        "index = pc.Index(\"document-embeddings\")\n",
        "\n",
        "# Load the model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "h5o6mphxExEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_arxiv_AI():\n",
        "  cols = ['id','submitter','authors', 'title', 'doi', 'abstract', 'categories', 'update_date']\n",
        "  data = []\n",
        "  file_name = '/content/dataset/arxiv-metadata-oai-snapshot.json'\n",
        "\n",
        "  with open(file_name, encoding='latin-1') as f:\n",
        "      for line in f:\n",
        "          doc = json.loads(line)\n",
        "          lst = [doc['id'], doc['submitter'], doc['authors'], doc['title'],\\\n",
        "                doc['doi'], doc['abstract'], doc['categories'], doc['update_date']]\n",
        "          data.append(lst)\n",
        "\n",
        "  df_data = pd.DataFrame(data=data, columns=cols)\n",
        "  ai_categories = ['cs.AI', 'cs.LG', 'cs.CL', 'cs.CV', 'stat.ML', 'cs.NE', 'eess.AS', 'stat.TH']\n",
        "  df_ai = df_data[df_data['categories'].apply(lambda x: any(cat in x for cat in ai_categories))]\n",
        "  return df_ai\n",
        "\n",
        "# Function to load the last update date from a file\n",
        "def load_last_update_date():\n",
        "    last_update_file = \"last_update_date.json\"\n",
        "    try:\n",
        "        with open(last_update_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            return datetime.strptime(data[\"last_update_date\"], \"%Y-%m-%d\")\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        return None\n",
        "\n",
        "# Function to save the last update date to a file\n",
        "def save_last_update_date(last_update_date):\n",
        "    last_update_file = \"last_update_date.json\"\n",
        "    with open(last_update_file, 'w') as f:\n",
        "        # Extract the 'update_date' value from the DataFrame\n",
        "        last_update_date_str = pd.to_datetime(last_update_date['update_date'].iloc[0]).strftime(\"%Y-%m-%d\")\n",
        "        json.dump({\"last_update_date\": last_update_date_str}, f)\n",
        "\n",
        "\n",
        "# Function to filter new articles based on update date and categories\n",
        "def filter_new_articles(df, last_update_date):\n",
        "    # Convert update_date column to datetime for comparison\n",
        "    df['update_date'] = pd.to_datetime(df['update_date'], format=\"%Y-%m-%d\")\n",
        "\n",
        "    # Filter by update date\n",
        "    if last_update_date:\n",
        "        df = df[df['update_date'] > last_update_date]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to encode and upsert new articles to Pinecone\n",
        "def upsert_new_articles_to_pinecone(df):\n",
        "    # Prepare embeddings and metadata for new articles\n",
        "    df['prepared_text'] = df['title'] + ' {title} ' + df['abstract']\n",
        "    embeddings = model.encode(df['prepared_text'].tolist(), batch_size=32, show_progress_bar=True)\n",
        "\n",
        "    # Create the document embedding dictionary\n",
        "    document_embedding_dict = {}\n",
        "    for i, row in df.reset_index(drop=True).iterrows():\n",
        "      doc_id = row['id']\n",
        "      update_date = row['update_date']\n",
        "      categories = row['categories'].split(';')\n",
        "      title = row['title']\n",
        "      abstract = row['abstract']\n",
        "      embedding = embeddings[i]\n",
        "\n",
        "      document_embedding_dict[doc_id] = {\n",
        "          'embedding': embedding,\n",
        "          'categories': categories,\n",
        "          'title': title,\n",
        "          'abstract': abstract,\n",
        "          'update_date': update_date\n",
        "      }\n",
        "\n",
        "    print(document_embedding_dict)\n",
        "    # Upsert document embeddings into Pinecone\n",
        "    batch_size = 100\n",
        "    embeddings_batch = []\n",
        "    for i, (doc_id, data) in enumerate(document_embedding_dict.items()):\n",
        "        embedding = data['embedding']\n",
        "        categories = data['categories']\n",
        "        update_date = str(data['update_date'])\n",
        "        title = data.get('title', '')\n",
        "        abstract = data.get('abstract', '')\n",
        "        embeddings_batch.append((doc_id, embedding.tolist(), {'categories': categories, 'update_date': update_date, 'title': title, 'abstract': abstract}))\n",
        "\n",
        "        # Upsert in batches\n",
        "        if (i + 1) % batch_size == 0 or i == len(document_embedding_dict) - 1:\n",
        "            index.upsert(embeddings_batch)\n",
        "            print(f\"Upserted batch {i // batch_size + 1} with {len(embeddings_batch)} documents.\")\n",
        "            embeddings_batch = []\n",
        "\n",
        "def update_pinecone_with_new_articles():\n",
        "    #load arxiv data from kaggle\n",
        "    df_ai = load_arxiv_AI()\n",
        "\n",
        "    # Get the last update date from Pinecone\n",
        "    last_update_date = load_last_update_date()\n",
        "    print(f\"Last update date in Pinecone: {last_update_date}\")\n",
        "\n",
        "    # Filter by update date\n",
        "    new_articles_df = filter_new_articles(df_ai, last_update_date)\n",
        "    print(f\"Number of new articles to upsert: {new_articles_df.shape[0]}\")\n",
        "\n",
        "    # Encode and upsert new articles to Pinecone\n",
        "    if not new_articles_df.empty:\n",
        "        upsert_new_articles_to_pinecone(new_articles_df)\n",
        "    else:\n",
        "        print(\"No new articles to upsert.\")\n",
        "\n",
        "    # Update the update_date json\n",
        "    new_articles_df['update_date'] = pd.to_datetime(new_articles_df['update_date'])\n",
        "\n",
        "    # The last paper based on the date\n",
        "    df_sorted_by_date = new_articles_df.sort_values(by='update_date')\n",
        "    last_article_by_date = df_sorted_by_date.tail(1)\n",
        "    save_last_update_date(last_article_by_date)\n",
        "\n"
      ],
      "metadata": {
        "id": "OHOjS3mvEynK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_pinecone_with_new_articles()"
      ],
      "metadata": {
        "id": "5nz4uemqE3VW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}